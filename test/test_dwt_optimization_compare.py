"""
DWT Prompt Generator ä¼˜åŒ–å‰åæ€§èƒ½å¯¹æ¯”æµ‹è¯•
========================================

å¯¹æ¯”åŸç‰ˆå’Œä¼˜åŒ–ç‰ˆçš„æ€§èƒ½å·®å¼‚

æ³¨æ„: ä½¿ç”¨ DWTPromptGenerator_performance_up è¿›è¡Œæµ‹è¯•
"""

import torch
import sys
import os
import time
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from layers.DWTPromptGenerator_performance_up import DWTPromptGenerator


def benchmark(generator, x_enc, n_runs=50, warmup=5):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    # é¢„çƒ­
    for _ in range(warmup):
        _ = generator(x_enc)
    
    # æµ‹è¯•
    times = []
    for _ in range(n_runs):
        start = time.perf_counter()
        _ = generator(x_enc)
        end = time.perf_counter()
        times.append((end - start) * 1000)
    
    return {
        'mean': np.mean(times),
        'std': np.std(times),
        'min': np.min(times),
        'max': np.max(times)
    }


def main():
    print("=" * 80)
    print("DWT Prompt Generator ä¼˜åŒ–å‰åæ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    
    # æµ‹è¯•é…ç½®
    batch_sizes = [1, 4, 8, 16, 32]
    seq_len = 512
    n_vars = 7
    
    print(f"\né…ç½®: seq_len={seq_len}, n_vars={n_vars}")
    print(f"{'Batch':<8} {'ä¼˜åŒ–ç‰ˆ(ms)':<15} {'ç¼–è¯‘ç‰ˆ(ms)':<15} {'åŠ é€Ÿæ¯”':<12} {'vsç¼–è¯‘':<12}")
    print("-" * 80)
    
    results = []
    
    for batch_size in batch_sizes:
        x_enc = torch.randn(batch_size, n_vars, seq_len)
        
        # æµ‹è¯•ä¼˜åŒ–ç‰ˆ (ä¸ä½¿ç”¨compile)
        generator_opt = DWTPromptGenerator(compression_level='balanced', use_compile=False)
        stats_opt = benchmark(generator_opt, x_enc)
        
        # æµ‹è¯•ç¼–è¯‘ç‰ˆ (ä½¿ç”¨compile)
        generator_compile = DWTPromptGenerator(compression_level='balanced', use_compile=True)
        stats_compile = benchmark(generator_compile, x_enc)
        
        speedup_vs_compile = stats_opt['mean'] / stats_compile['mean']
        
        results.append({
            'batch_size': batch_size,
            'optimized': stats_opt['mean'],
            'compiled': stats_compile['mean'],
            'speedup_compile': speedup_vs_compile
        })
        
        print(f"{batch_size:<8} {stats_opt['mean']:<15.2f} {stats_compile['mean']:<15.2f} "
              f"{'N/A':<12} {speedup_vs_compile:<12.2f}x")
    
    # GPUæµ‹è¯•
    if torch.cuda.is_available():
        print("\n" + "=" * 80)
        print("GPUæ€§èƒ½æµ‹è¯•")
        print("=" * 80)
        
        print(f"\n{'Batch':<8} {'CPUä¼˜åŒ–(ms)':<15} {'GPUä¼˜åŒ–(ms)':<15} {'GPUåŠ é€Ÿæ¯”':<12}")
        print("-" * 70)
        
        for batch_size in [4, 8, 16, 32]:
            # CPUæµ‹è¯•
            x_enc_cpu = torch.randn(batch_size, n_vars, seq_len)
            generator_cpu = DWTPromptGenerator(compression_level='balanced', use_compile=False)
            stats_cpu = benchmark(generator_cpu, x_enc_cpu, n_runs=20)
            
            # GPUæµ‹è¯•
            x_enc_gpu = torch.randn(batch_size, n_vars, seq_len).cuda()
            generator_gpu = DWTPromptGenerator(compression_level='balanced', use_compile=False).cuda()
            stats_gpu = benchmark(generator_gpu, x_enc_gpu, n_runs=20)
            
            speedup = stats_cpu['mean'] / stats_gpu['mean']
            
            print(f"{batch_size:<8} {stats_cpu['mean']:<15.2f} {stats_gpu['mean']:<15.2f} {speedup:<12.2f}x")
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("æ€§èƒ½æ€»ç»“")
    print("=" * 80)
    
    avg_compile_speedup = np.mean([r['speedup_compile'] for r in results])
    
    print(f"\nâœ… ä¼˜åŒ–æˆæœ:")
    print(f"   - å¹³å‡torch.compileåŠ é€Ÿæ¯”: {avg_compile_speedup:.2f}x")
    print(f"   - æ¨èé…ç½®: use_compile=True (PyTorch 2.0+)")
    
    print("\nğŸ“Š å…³é”®æ”¹è¿›:")
    print("   1. âœ… å‘é‡åŒ–èƒ½é‡è®¡ç®— - å‡å°‘å¾ªç¯å¼€é”€")
    print("   2. âœ… å‡å°‘CPU-GPUæ•°æ®ä¼ è¾“ - ä¿æŒtensoråœ¨è®¾å¤‡ä¸Š")
    print("   3. âœ… torch.compileåŠ é€Ÿ - JITç¼–è¯‘ä¼˜åŒ–")
    print("   4. âœ… æ‰¹é‡åŒ–è¶‹åŠ¿è®¡ç®— - æå‡å¹¶è¡Œåº¦")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()
