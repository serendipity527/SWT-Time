"""
DWT Prompt Generator æ€§èƒ½åˆ†æå’Œæµ‹è¯•å¥—ä»¶
==========================================

è¯¥è„šæœ¬å…¨é¢æµ‹è¯•DWTPromptGeneratoræ¨¡å—çš„æ€§èƒ½ç‰¹å¾:
1. æ—¶é—´å¤æ‚åº¦åˆ†æ
2. ç©ºé—´å¤æ‚åº¦åˆ†æ
3. ä¸åŒé…ç½®çš„æ€§èƒ½å¯¹æ¯”
4. GPU vs CPUæ€§èƒ½å¯¹æ¯”
5. å†…å­˜æ•ˆç‡åˆ†æ
6. ç“¶é¢ˆè¯†åˆ«

æ³¨æ„: ä½¿ç”¨ DWTPromptGenerator_performance_up è¿›è¡Œæµ‹è¯•
"""

import torch
import sys
import os
import time
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import tracemalloc
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from layers.DWTPromptGenerator_performance_up import DWTPromptGenerator


class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.results = defaultdict(list)
    
    def measure_time(self, func, *args, n_runs=10, warmup=2):
        """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´"""
        # é¢„çƒ­
        for _ in range(warmup):
            func(*args)
        
        # æµ‹é‡
        times = []
        for _ in range(n_runs):
            start = time.perf_counter()
            result = func(*args)
            end = time.perf_counter()
            times.append((end - start) * 1000)  # ms
        
        return {
            'mean': np.mean(times),
            'std': np.std(times),
            'min': np.min(times),
            'max': np.max(times),
            'median': np.median(times)
        }
    
    def measure_memory(self, func, *args):
        """æµ‹é‡å†…å­˜ä½¿ç”¨"""
        tracemalloc.start()
        result = func(*args)
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        return {
            'current_mb': current / 1024 / 1024,
            'peak_mb': peak / 1024 / 1024
        }


def test_time_complexity():
    """æµ‹è¯•1: æ—¶é—´å¤æ‚åº¦åˆ†æ"""
    print("=" * 80)
    print("æµ‹è¯•1: æ—¶é—´å¤æ‚åº¦åˆ†æ")
    print("=" * 80)
    
    profiler = PerformanceProfiler()
    generator = DWTPromptGenerator(compression_level='balanced')
    
    # æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦
    print("\n1.1 ä¸åŒåºåˆ—é•¿åº¦çš„æ€§èƒ½ (å›ºå®š batch_size=4, n_vars=7)")
    print(f"{'åºåˆ—é•¿åº¦':<12} {'å¹³å‡æ—¶é—´(ms)':<15} {'æ ‡å‡†å·®':<12} {'ååé‡(æ ·æœ¬/s)':<15}")
    print("-" * 70)
    
    seq_lengths = [128, 256, 512, 1024, 2048, 4096]
    time_results = []
    
    for seq_len in seq_lengths:
        x_enc = torch.randn(4, 7, seq_len)
        
        stats = profiler.measure_time(generator, x_enc, n_runs=20)
        throughput = 4 / (stats['mean'] / 1000)
        
        time_results.append({
            'seq_len': seq_len,
            'time': stats['mean'],
            'std': stats['std'],
            'throughput': throughput
        })
        
        print(f"{seq_len:<12} {stats['mean']:<15.2f} {stats['std']:<12.2f} {throughput:<15.1f}")
    
    # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
    print("\n1.2 ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½ (å›ºå®š seq_len=512, n_vars=7)")
    print(f"{'æ‰¹æ¬¡å¤§å°':<12} {'å¹³å‡æ—¶é—´(ms)':<15} {'æ ‡å‡†å·®':<12} {'ååé‡(æ ·æœ¬/s)':<15}")
    print("-" * 70)
    
    batch_results = []
    batch_sizes = [1, 2, 4, 8, 16, 32, 64]
    
    for batch_size in batch_sizes:
        x_enc = torch.randn(batch_size, 7, 512)
        
        stats = profiler.measure_time(generator, x_enc, n_runs=20)
        throughput = batch_size / (stats['mean'] / 1000)
        
        batch_results.append({
            'batch_size': batch_size,
            'time': stats['mean'],
            'std': stats['std'],
            'throughput': throughput
        })
        
        print(f"{batch_size:<12} {stats['mean']:<15.2f} {stats['std']:<12.2f} {throughput:<15.1f}")
    
    # æµ‹è¯•ä¸åŒå˜é‡æ•°
    print("\n1.3 ä¸åŒå˜é‡æ•°çš„æ€§èƒ½ (å›ºå®š batch_size=4, seq_len=512)")
    print(f"{'å˜é‡æ•°':<12} {'å¹³å‡æ—¶é—´(ms)':<15} {'æ ‡å‡†å·®':<12} {'ååé‡(æ ·æœ¬/s)':<15}")
    print("-" * 70)
    
    var_results = []
    n_vars_list = [1, 3, 7, 14, 21, 28]
    
    for n_vars in n_vars_list:
        x_enc = torch.randn(4, n_vars, 512)
        
        stats = profiler.measure_time(generator, x_enc, n_runs=20)
        throughput = 4 / (stats['mean'] / 1000)
        
        var_results.append({
            'n_vars': n_vars,
            'time': stats['mean'],
            'std': stats['std'],
            'throughput': throughput
        })
        
        print(f"{n_vars:<12} {stats['mean']:<15.2f} {stats['std']:<12.2f} {throughput:<15.1f}")
    
    print("\nâœ… æ—¶é—´å¤æ‚åº¦åˆ†æå®Œæˆ!")
    return time_results, batch_results, var_results


def test_space_complexity():
    """æµ‹è¯•2: ç©ºé—´å¤æ‚åº¦åˆ†æ"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•2: ç©ºé—´å¤æ‚åº¦åˆ†æ")
    print("=" * 80)
    
    profiler = PerformanceProfiler()
    generator = DWTPromptGenerator(compression_level='balanced')
    
    print("\n2.1 ä¸åŒåºåˆ—é•¿åº¦çš„å†…å­˜ä½¿ç”¨")
    print(f"{'åºåˆ—é•¿åº¦':<12} {'å³°å€¼å†…å­˜(MB)':<15} {'å½“å‰å†…å­˜(MB)':<15}")
    print("-" * 50)
    
    memory_results = []
    seq_lengths = [128, 256, 512, 1024, 2048, 4096]
    
    for seq_len in seq_lengths:
        x_enc = torch.randn(4, 7, seq_len)
        
        mem_stats = profiler.measure_memory(generator, x_enc)
        
        memory_results.append({
            'seq_len': seq_len,
            'peak_mb': mem_stats['peak_mb'],
            'current_mb': mem_stats['current_mb']
        })
        
        print(f"{seq_len:<12} {mem_stats['peak_mb']:<15.2f} {mem_stats['current_mb']:<15.2f}")
    
    # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„å†…å­˜
    print("\n2.2 ä¸åŒæ‰¹æ¬¡å¤§å°çš„å†…å­˜ä½¿ç”¨")
    print(f"{'æ‰¹æ¬¡å¤§å°':<12} {'å³°å€¼å†…å­˜(MB)':<15} {'å½“å‰å†…å­˜(MB)':<15}")
    print("-" * 50)
    
    batch_memory_results = []
    batch_sizes = [1, 4, 8, 16, 32, 64]
    
    for batch_size in batch_sizes:
        x_enc = torch.randn(batch_size, 7, 512)
        
        mem_stats = profiler.measure_memory(generator, x_enc)
        
        batch_memory_results.append({
            'batch_size': batch_size,
            'peak_mb': mem_stats['peak_mb'],
            'current_mb': mem_stats['current_mb']
        })
        
        print(f"{batch_size:<12} {mem_stats['peak_mb']:<15.2f} {mem_stats['current_mb']:<15.2f}")
    
    print("\nâœ… ç©ºé—´å¤æ‚åº¦åˆ†æå®Œæˆ!")
    return memory_results, batch_memory_results


def test_configuration_comparison():
    """æµ‹è¯•3: ä¸åŒé…ç½®çš„æ€§èƒ½å¯¹æ¯”"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•3: ä¸åŒé…ç½®çš„æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    
    profiler = PerformanceProfiler()
    x_enc = torch.randn(8, 7, 512)
    
    # æµ‹è¯•ä¸åŒå°æ³¢åŸº
    print("\n3.1 ä¸åŒå°æ³¢åŸºçš„æ€§èƒ½å¯¹æ¯”")
    print(f"{'å°æ³¢åŸº':<12} {'å¹³å‡æ—¶é—´(ms)':<15} {'æ ‡å‡†å·®':<12}")
    print("-" * 50)
    
    wavelets = ['db1', 'db4', 'db8', 'sym4', 'coif2']
    wavelet_results = []
    
    for wavelet in wavelets:
        try:
            generator = DWTPromptGenerator(wavelet=wavelet, level=3)
            stats = profiler.measure_time(generator, x_enc, n_runs=20)
            
            wavelet_results.append({
                'wavelet': wavelet,
                'time': stats['mean'],
                'std': stats['std']
            })
            
            print(f"{wavelet:<12} {stats['mean']:<15.2f} {stats['std']:<12.2f}")
        except Exception as e:
            print(f"{wavelet:<12} å¤±è´¥: {e}")
    
    # æµ‹è¯•ä¸åŒåˆ†è§£å±‚æ•°
    print("\n3.2 ä¸åŒåˆ†è§£å±‚æ•°çš„æ€§èƒ½å¯¹æ¯”")
    print(f"{'åˆ†è§£å±‚æ•°':<12} {'å¹³å‡æ—¶é—´(ms)':<15} {'æ ‡å‡†å·®':<12}")
    print("-" * 50)
    
    level_results = []
    levels = [1, 2, 3, 4, 5]
    
    for level in levels:
        try:
            generator = DWTPromptGenerator(wavelet='db4', level=level)
            stats = profiler.measure_time(generator, x_enc, n_runs=20)
            
            level_results.append({
                'level': level,
                'time': stats['mean'],
                'std': stats['std']
            })
            
            print(f"{level:<12} {stats['mean']:<15.2f} {stats['std']:<12.2f}")
        except Exception as e:
            print(f"{level:<12} å¤±è´¥: {e}")
    
    # æµ‹è¯•ä¸åŒå‹ç¼©çº§åˆ«
    print("\n3.3 ä¸åŒå‹ç¼©çº§åˆ«çš„Promptç”Ÿæˆæ€§èƒ½")
    print(f"{'å‹ç¼©çº§åˆ«':<12} {'å¹³å‡æ—¶é—´(ms)':<15} {'Tokenæ•°':<12}")
    print("-" * 50)
    
    compression_results = []
    compressions = ['minimal', 'balanced', 'detailed']
    
    base_info = {
        'min': -1.234,
        'max': 2.567,
        'median': 0.345,
        'lags': np.array([24, 48, 96, 168, 336]),
        'description': 'Test dataset',
        'seq_len': 512,
        'pred_len': 96
    }
    
    for compression in compressions:
        generator = DWTPromptGenerator(compression_level=compression)
        features = generator(x_enc)
        
        def build_prompt():
            return generator.build_prompt_text(features, base_info)
        
        stats = profiler.measure_time(build_prompt, n_runs=50)
        prompt = build_prompt()
        token_count = len(prompt.split())
        
        compression_results.append({
            'compression': compression,
            'time': stats['mean'],
            'tokens': token_count
        })
        
        print(f"{compression:<12} {stats['mean']:<15.2f} {token_count:<12}")
    
    print("\nâœ… é…ç½®å¯¹æ¯”æµ‹è¯•å®Œæˆ!")
    return wavelet_results, level_results, compression_results


def test_gpu_vs_cpu():
    """æµ‹è¯•4: GPU vs CPUæ€§èƒ½å¯¹æ¯”"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•4: GPU vs CPUæ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
        return None
    
    profiler = PerformanceProfiler()
    
    print("\n4.1 ä¸åŒæ‰¹æ¬¡å¤§å°çš„GPUåŠ é€Ÿæ¯”")
    print(f"{'æ‰¹æ¬¡å¤§å°':<12} {'CPU(ms)':<12} {'GPU(ms)':<12} {'åŠ é€Ÿæ¯”':<12}")
    print("-" * 60)
    
    gpu_results = []
    batch_sizes = [1, 4, 8, 16, 32, 64]
    
    for batch_size in batch_sizes:
        # CPUæµ‹è¯•
        generator_cpu = DWTPromptGenerator(compression_level='balanced')
        x_enc_cpu = torch.randn(batch_size, 7, 512)
        cpu_stats = profiler.measure_time(generator_cpu, x_enc_cpu, n_runs=20)
        
        # GPUæµ‹è¯•
        generator_gpu = DWTPromptGenerator(compression_level='balanced').cuda()
        x_enc_gpu = torch.randn(batch_size, 7, 512).cuda()
        gpu_stats = profiler.measure_time(generator_gpu, x_enc_gpu, n_runs=20)
        
        speedup = cpu_stats['mean'] / gpu_stats['mean']
        
        gpu_results.append({
            'batch_size': batch_size,
            'cpu_time': cpu_stats['mean'],
            'gpu_time': gpu_stats['mean'],
            'speedup': speedup
        })
        
        print(f"{batch_size:<12} {cpu_stats['mean']:<12.2f} {gpu_stats['mean']:<12.2f} {speedup:<12.2f}x")
    
    print("\nâœ… GPU vs CPUå¯¹æ¯”æµ‹è¯•å®Œæˆ!")
    return gpu_results


def test_bottleneck_analysis():
    """æµ‹è¯•5: ç“¶é¢ˆè¯†åˆ«"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•5: ç“¶é¢ˆè¯†åˆ«")
    print("=" * 80)
    
    import time
    
    generator = DWTPromptGenerator(compression_level='balanced')
    x_enc = torch.randn(8, 7, 512)
    
    print("\n5.1 å„é˜¶æ®µè€—æ—¶åˆ†æ")
    
    # æ‰‹åŠ¨è®¡æ—¶å„ä¸ªé˜¶æ®µ
    B, N, T = x_enc.shape
    x_reshaped = x_enc.reshape(B * N, 1, T).float()
    
    # DWTåˆ†è§£
    import ptwt
    start = time.perf_counter()
    for _ in range(50):
        coeffs = ptwt.wavedec(x_reshaped, 'db4', level=3, mode='reflect')
    dwt_time = (time.perf_counter() - start) / 50 * 1000
    
    # é¢‘åŸŸç‰¹å¾æå–
    start = time.perf_counter()
    for _ in range(50):
        freq_features = generator._extract_frequency_features(coeffs)
    freq_time = (time.perf_counter() - start) / 50 * 1000
    
    # è¶‹åŠ¿ç‰¹å¾æå–
    start = time.perf_counter()
    for _ in range(50):
        trend_features = generator._extract_trend_features(coeffs)
    trend_time = (time.perf_counter() - start) / 50 * 1000
    
    # è´¨é‡ç‰¹å¾æå–
    start = time.perf_counter()
    for _ in range(50):
        quality_features = generator._extract_quality_features(coeffs)
    quality_time = (time.perf_counter() - start) / 50 * 1000
    
    # éš¾åº¦è®¡ç®—
    start = time.perf_counter()
    for _ in range(50):
        difficulty = generator._calculate_difficulty(freq_features, trend_features, quality_features)
    difficulty_time = (time.perf_counter() - start) / 50 * 1000
    
    # æ€»æ—¶é—´
    total_time = dwt_time + freq_time + trend_time + quality_time + difficulty_time
    
    # æ‰“å°ç»“æœ
    stages = [
        ('DWTåˆ†è§£', dwt_time),
        ('é¢‘åŸŸç‰¹å¾æå–', freq_time),
        ('è¶‹åŠ¿ç‰¹å¾æå–', trend_time),
        ('è´¨é‡ç‰¹å¾æå–', quality_time),
        ('éš¾åº¦è®¡ç®—', difficulty_time)
    ]
    
    print(f"{'é˜¶æ®µ':<20} {'æ—¶é—´(ms)':<12} {'å æ¯”':<12}")
    print("-" * 50)
    
    bottleneck_results = []
    for stage_name, stage_time in stages:
        percentage = (stage_time / total_time) * 100
        bottleneck_results.append({
            'stage': stage_name,
            'time': stage_time,
            'percentage': percentage
        })
        print(f"{stage_name:<20} {stage_time:<12.3f} {percentage:<12.1f}%")
    
    print(f"{'æ€»è®¡':<20} {total_time:<12.3f} {100.0:<12.1f}%")
    
    print("\nâœ… ç“¶é¢ˆè¯†åˆ«å®Œæˆ!")
    return bottleneck_results


def test_scalability():
    """æµ‹è¯•6: å¯æ‰©å±•æ€§åˆ†æ"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•6: å¯æ‰©å±•æ€§åˆ†æ")
    print("=" * 80)
    
    profiler = PerformanceProfiler()
    generator = DWTPromptGenerator(compression_level='balanced')
    
    print("\n6.1 å¤§è§„æ¨¡æ‰¹å¤„ç†æ€§èƒ½")
    print(f"{'æ€»æ ·æœ¬æ•°':<15} {'æ‰¹æ¬¡å¤§å°':<12} {'æ—¶é—´(ms)':<12} {'ååé‡(æ ·æœ¬/s)':<15}")
    print("-" * 70)
    
    scalability_results = []
    
    # å›ºå®šæ€»æ ·æœ¬æ•°ï¼Œæµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
    total_samples = 128
    batch_sizes = [1, 4, 8, 16, 32, 64, 128]
    
    for batch_size in batch_sizes:
        if batch_size > total_samples:
            continue
            
        n_batches = total_samples // batch_size
        x_enc = torch.randn(batch_size, 7, 512)
        
        start = time.perf_counter()
        for _ in range(n_batches):
            _ = generator(x_enc)
        elapsed = (time.perf_counter() - start) * 1000
        
        throughput = total_samples / (elapsed / 1000)
        
        scalability_results.append({
            'total_samples': total_samples,
            'batch_size': batch_size,
            'time': elapsed,
            'throughput': throughput
        })
        
        print(f"{total_samples:<15} {batch_size:<12} {elapsed:<12.2f} {throughput:<15.1f}")
    
    print("\nâœ… å¯æ‰©å±•æ€§åˆ†æå®Œæˆ!")
    return scalability_results


def generate_summary_report(all_results):
    """ç”Ÿæˆæ€§èƒ½åˆ†ææ€»ç»“æŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("æ€§èƒ½åˆ†ææ€»ç»“æŠ¥å‘Š")
    print("=" * 80)
    
    print("\n## ä¸»è¦å‘ç°\n")
    
    # 1. æ—¶é—´å¤æ‚åº¦ç»“è®º
    time_results, batch_results, var_results = all_results['time_complexity']
    print("### 1. æ—¶é—´å¤æ‚åº¦")
    print(f"   - åºåˆ—é•¿åº¦å½±å“: ä»128åˆ°4096ï¼Œæ—¶é—´å¢é•¿çº¦ {time_results[-1]['time']/time_results[0]['time']:.1f}x")
    print(f"   - æ‰¹æ¬¡å¤§å°å½±å“: æ‰¹å¤„ç†æ•ˆç‡éšæ‰¹æ¬¡å¢å¤§è€Œæå‡")
    print(f"   - å˜é‡æ•°å½±å“: çº¿æ€§å¢é•¿å…³ç³»")
    
    # 2. ç©ºé—´å¤æ‚åº¦ç»“è®º
    memory_results, batch_memory_results = all_results['space_complexity']
    print("\n### 2. ç©ºé—´å¤æ‚åº¦")
    print(f"   - å†…å­˜ä½¿ç”¨éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿")
    print(f"   - å…¸å‹é…ç½®(B=4,N=7,T=512)å†…å­˜ä½¿ç”¨: ~{memory_results[2]['peak_mb']:.1f}MB")
    
    # 3. æœ€ä¼˜é…ç½®
    wavelet_results, level_results, compression_results = all_results['config_comparison']
    print("\n### 3. æœ€ä¼˜é…ç½®")
    best_wavelet = min(wavelet_results, key=lambda x: x['time'])
    print(f"   - æœ€å¿«å°æ³¢åŸº: {best_wavelet['wavelet']} ({best_wavelet['time']:.2f}ms)")
    best_level = min(level_results, key=lambda x: x['time'])
    print(f"   - æœ€å¿«åˆ†è§£å±‚æ•°: {best_level['level']} ({best_level['time']:.2f}ms)")
    print(f"   - æ¨èå‹ç¼©çº§åˆ«: balanced (å¹³è¡¡æ€§èƒ½å’Œä¿¡æ¯é‡)")
    
    # 4. ç“¶é¢ˆ
    bottleneck_results = all_results['bottleneck']
    max_bottleneck = max(bottleneck_results, key=lambda x: x['percentage'])
    print("\n### 4. æ€§èƒ½ç“¶é¢ˆ")
    print(f"   - ä¸»è¦ç“¶é¢ˆ: {max_bottleneck['stage']} ({max_bottleneck['percentage']:.1f}%)")
    print(f"   - ä¼˜åŒ–å»ºè®®: é‡ç‚¹ä¼˜åŒ–DWTåˆ†è§£å’Œç‰¹å¾æå–å¹¶è¡ŒåŒ–")
    
    # 5. GPUåŠ é€Ÿ
    if all_results['gpu_comparison'] is not None:
        gpu_results = all_results['gpu_comparison']
        avg_speedup = np.mean([r['speedup'] for r in gpu_results])
        print("\n### 5. GPUåŠ é€Ÿæ•ˆæœ")
        print(f"   - å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.2f}x")
        print(f"   - GPUåŠ é€Ÿå»ºè®®: æ‰¹æ¬¡å¤§å° >= 8 æ—¶æ•ˆæœæ˜æ˜¾")
    
    # 6. å¯æ‰©å±•æ€§
    scalability_results = all_results['scalability']
    best_throughput = max(scalability_results, key=lambda x: x['throughput'])
    print("\n### 6. å¯æ‰©å±•æ€§")
    print(f"   - æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {best_throughput['batch_size']} (ååé‡: {best_throughput['throughput']:.1f} æ ·æœ¬/ç§’)")
    print(f"   - å»ºè®®: ä½¿ç”¨ä¸­ç­‰æ‰¹æ¬¡å¤§å°(8-32)è·å¾—æœ€ä½³æ€§èƒ½")
    
    print("\n" + "=" * 80)


def save_results_to_json(all_results, filename='dwt_performance_results.json'):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
    def convert_to_serializable(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [convert_to_serializable(item) for item in obj]
        return obj
    
    serializable_results = convert_to_serializable(all_results)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {filename}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("DWT Prompt Generator æ€§èƒ½åˆ†æå¥—ä»¶")
    print("=" * 80)
    print("å¼€å§‹å…¨é¢æ€§èƒ½æµ‹è¯•...\n")
    
    all_results = {}
    
    try:
        # æµ‹è¯•1: æ—¶é—´å¤æ‚åº¦
        time_results, batch_results, var_results = test_time_complexity()
        all_results['time_complexity'] = (time_results, batch_results, var_results)
        
        # æµ‹è¯•2: ç©ºé—´å¤æ‚åº¦
        memory_results, batch_memory_results = test_space_complexity()
        all_results['space_complexity'] = (memory_results, batch_memory_results)
        
        # æµ‹è¯•3: é…ç½®å¯¹æ¯”
        wavelet_results, level_results, compression_results = test_configuration_comparison()
        all_results['config_comparison'] = (wavelet_results, level_results, compression_results)
        
        # æµ‹è¯•4: GPUå¯¹æ¯”
        gpu_results = test_gpu_vs_cpu()
        all_results['gpu_comparison'] = gpu_results
        
        # æµ‹è¯•5: ç“¶é¢ˆè¯†åˆ«
        bottleneck_results = test_bottleneck_analysis()
        all_results['bottleneck'] = bottleneck_results
        
        # æµ‹è¯•6: å¯æ‰©å±•æ€§
        scalability_results = test_scalability()
        all_results['scalability'] = scalability_results
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        generate_summary_report(all_results)
        
        # ä¿å­˜ç»“æœ
        save_results_to_json(all_results)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ æ€§èƒ½åˆ†æå®Œæˆ!")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()
