"""
TimeLLM + DWT Prompt é›†æˆæµ‹è¯•
æµ‹è¯•DWT Promptç”Ÿæˆå™¨ä¸TimeLLMæ¨¡å‹çš„å®Œæ•´é›†æˆ
"""

import torch
import sys
import os
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.TimeLLM import Model


class MockConfigs:
    """æ¨¡æ‹Ÿé…ç½®å¯¹è±¡ï¼Œç”¨äºæµ‹è¯•"""
    def __init__(self, use_dwt_prompt=False, prompt_compression='balanced'):
        # åŸºç¡€é…ç½®
        self.task_name = 'long_term_forecast'
        self.pred_len = 96
        self.seq_len = 512
        self.label_len = 48
        self.d_model = 16
        self.d_ff = 32
        self.n_heads = 8
        self.enc_in = 7
        self.dec_in = 7
        self.c_out = 7
        self.dropout = 0.1
        self.llm_layers = 6
        self.llm_dim = 768
        self.patch_len = 16
        self.stride = 8
        
        # SWTé…ç½®
        self.use_swt = False
        self.swt_wavelet = 'db4'
        self.swt_level = 3
        self.use_all_coeffs = True
        
        # DWT Prompté…ç½®
        self.use_dwt_prompt = use_dwt_prompt
        self.dwt_prompt_level = 3
        self.prompt_compression = prompt_compression
        
        # LLMé…ç½®
        self.llm_model = 'GPT2'  # ä½¿ç”¨GPT2æ›´å¿«
        
        # Prompté…ç½®
        self.prompt_domain = False
        self.content = 'Test dataset'


def test_model_initialization():
    """æµ‹è¯•1: æ¨¡å‹åˆå§‹åŒ–"""
    print("=" * 80)
    print("æµ‹è¯•1: TimeLLMæ¨¡å‹åˆå§‹åŒ–æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•åŸç‰ˆé…ç½®
    print("\n[1.1] æµ‹è¯•åŸç‰ˆé…ç½®ï¼ˆuse_dwt_prompt=Falseï¼‰")
    configs_baseline = MockConfigs(use_dwt_prompt=False)
    try:
        model_baseline = Model(configs_baseline)
        assert model_baseline.dwt_prompt_generator is None
        print("âœ… åŸç‰ˆé…ç½®åˆå§‹åŒ–æˆåŠŸï¼Œdwt_prompt_generator=None")
    except Exception as e:
        print(f"âŒ åŸç‰ˆé…ç½®åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    # æµ‹è¯•DWTé…ç½®
    print("\n[1.2] æµ‹è¯•DWTé…ç½®ï¼ˆuse_dwt_prompt=Trueï¼‰")
    configs_dwt = MockConfigs(use_dwt_prompt=True, prompt_compression='balanced')
    try:
        model_dwt = Model(configs_dwt)
        assert model_dwt.dwt_prompt_generator is not None
        assert model_dwt.use_dwt_prompt == True
        assert model_dwt.prompt_compression == 'balanced'
        print("âœ… DWTé…ç½®åˆå§‹åŒ–æˆåŠŸï¼Œdwt_prompt_generatorå·²åˆ›å»º")
    except Exception as e:
        print(f"âŒ DWTé…ç½®åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    # æµ‹è¯•ä¸åŒå‹ç¼©çº§åˆ«
    print("\n[1.3] æµ‹è¯•ä¸åŒå‹ç¼©çº§åˆ«")
    for compression in ['minimal', 'balanced', 'detailed']:
        configs = MockConfigs(use_dwt_prompt=True, prompt_compression=compression)
        model = Model(configs)
        assert model.prompt_compression == compression
        print(f"âœ… {compression} çº§åˆ«åˆå§‹åŒ–æˆåŠŸ")
    
    print("\nâœ… æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•é€šè¿‡!")


def test_forward_pass():
    """æµ‹è¯•2: Forward Pass"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•2: Forward Passæµ‹è¯•")
    print("=" * 80)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    batch_size = 2
    seq_len = 512
    pred_len = 96
    label_len = 48
    enc_in = 7
    
    x_enc = torch.randn(batch_size, seq_len, enc_in)
    x_mark_enc = torch.zeros(batch_size, seq_len, 4)  # æ—¶é—´ç‰¹å¾
    x_dec = torch.randn(batch_size, label_len + pred_len, enc_in)
    x_mark_dec = torch.zeros(batch_size, label_len + pred_len, 4)
    
    print(f"\nè¾“å…¥æ•°æ®å½¢çŠ¶:")
    print(f"  x_enc: {x_enc.shape}")
    print(f"  x_dec: {x_dec.shape}")
    
    # æµ‹è¯•åŸç‰ˆforward
    print("\n[2.1] æµ‹è¯•åŸç‰ˆForward Pass")
    configs_baseline = MockConfigs(use_dwt_prompt=False)
    model_baseline = Model(configs_baseline)
    model_baseline.eval()
    
    try:
        with torch.no_grad():
            output_baseline = model_baseline(x_enc, x_mark_enc, x_dec, x_mark_dec)
        print(f"âœ… åŸç‰ˆForwardé€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {output_baseline.shape}")
        assert output_baseline.shape == (batch_size, pred_len, enc_in)
    except Exception as e:
        print(f"âŒ åŸç‰ˆForwardå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    # æµ‹è¯•DWT Forward
    print("\n[2.2] æµ‹è¯•DWT Forward Pass")
    configs_dwt = MockConfigs(use_dwt_prompt=True, prompt_compression='balanced')
    model_dwt = Model(configs_dwt)
    model_dwt.eval()
    
    try:
        with torch.no_grad():
            output_dwt = model_dwt(x_enc, x_mark_enc, x_dec, x_mark_dec)
        print(f"âœ… DWT Forwardé€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {output_dwt.shape}")
        assert output_dwt.shape == (batch_size, pred_len, enc_in)
    except Exception as e:
        print(f"âŒ DWT Forwardå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    print("\nâœ… Forward Passæµ‹è¯•é€šè¿‡!")


def test_prompt_generation():
    """æµ‹è¯•3: Promptç”Ÿæˆå¯¹æ¯”"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•3: Promptç”Ÿæˆå¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    
    # å‡†å¤‡æ•°æ®
    batch_size = 2
    seq_len = 512
    enc_in = 7
    
    x_enc = torch.randn(batch_size, seq_len, enc_in)
    
    # åˆ›å»ºæ¨¡å‹
    configs_baseline = MockConfigs(use_dwt_prompt=False)
    configs_dwt = MockConfigs(use_dwt_prompt=True, prompt_compression='balanced')
    
    model_baseline = Model(configs_baseline)
    model_dwt = Model(configs_dwt)
    
    print("\n[3.1] åŸç‰ˆPromptç”Ÿæˆ")
    # æ‰‹åŠ¨è°ƒç”¨forecastæ–¹æ³•çš„promptç”Ÿæˆéƒ¨åˆ†æ¥æ•è·prompt
    model_baseline.eval()
    
    # æ¨¡æ‹Ÿforecastä¸­çš„promptç”Ÿæˆé€»è¾‘
    x_enc_normalized = model_baseline.normalize_layers(x_enc, 'norm')
    B, T, N = x_enc_normalized.size()
    x_enc_reshaped = x_enc_normalized.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)
    
    min_values = torch.min(x_enc_reshaped, dim=1)[0]
    max_values = torch.max(x_enc_reshaped, dim=1)[0]
    medians = torch.median(x_enc_reshaped, dim=1).values
    lags = model_baseline.calcute_lags(x_enc_reshaped)
    trends = x_enc_reshaped.diff(dim=1).sum(dim=1)
    
    # ç”ŸæˆåŸç‰ˆpromptï¼ˆåªå–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
    prompt_baseline = (
        f"<|start_prompt|>Dataset description: {model_baseline.description}"
        f"Task description: forecast the next {model_baseline.pred_len} steps given the previous {model_baseline.seq_len} steps information; "
        f"Input statistics: min value {min_values[0].item():.4f}, max value {max_values[0].item():.4f}, "
        f"median value {medians[0].item():.4f}, the trend of input is {'upward' if trends[0] > 0 else 'downward'}, "
        f"top 5 lags are : {lags[0].tolist()}<|<end_prompt>|>"
    )
    
    print(f"åŸç‰ˆPrompt (æ ·æœ¬0):")
    print(prompt_baseline)
    print(f"Tokenæ•°ä¼°è®¡: {len(prompt_baseline.split())}")
    
    print("\n[3.2] DWT Promptç”Ÿæˆ")
    # ä½¿ç”¨DWTç”Ÿæˆå™¨
    x_sample = x_enc_normalized[0:1, :, :].permute(0, 2, 1)  # (1, N, T)
    dwt_features = model_dwt.dwt_prompt_generator(x_sample)
    
    base_info = {
        'min': min_values[0].item(),
        'max': max_values[0].item(),
        'median': medians[0].item(),
        'lags': lags[0].cpu().numpy(),
        'description': model_dwt.description,
        'seq_len': model_dwt.seq_len,
        'pred_len': model_dwt.pred_len
    }
    
    prompt_dwt = model_dwt.dwt_prompt_generator.build_prompt_text(dwt_features, base_info)
    
    print(f"DWT Prompt (æ ·æœ¬0, {model_dwt.prompt_compression}æ¨¡å¼):")
    print(prompt_dwt)
    print(f"Tokenæ•°ä¼°è®¡: {len(prompt_dwt.split())}")
    
    print("\n[3.3] å¯¹æ¯”åˆ†æ")
    print(f"åŸç‰ˆTokenæ•°: {len(prompt_baseline.split())}")
    print(f"DWT Tokenæ•°: {len(prompt_dwt.split())}")
    print(f"Tokenå¢åŠ : +{len(prompt_dwt.split()) - len(prompt_baseline.split())} "
          f"({(len(prompt_dwt.split()) - len(prompt_baseline.split())) / len(prompt_baseline.split()) * 100:.1f}%)")
    
    print("\nDWTæ–°å¢ä¿¡æ¯:")
    print(f"  - é¢‘åŸŸæ¨¡å¼: {dwt_features['freq_pattern']}")
    print(f"  - è¶‹åŠ¿ç»†åŒ–: {dwt_features['trend_desc']}")
    print(f"  - ä¿¡å·è´¨é‡: {dwt_features['signal_quality']} (SNR: {dwt_features['snr_db']:.1f} dB)")
    print(f"  - é¢„æµ‹éš¾åº¦: {dwt_features['difficulty']}")
    
    print("\nâœ… Promptç”Ÿæˆå¯¹æ¯”æµ‹è¯•é€šè¿‡!")


def test_batch_compatibility():
    """æµ‹è¯•4: æ‰¹å¤„ç†å…¼å®¹æ€§"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•4: æ‰¹å¤„ç†å…¼å®¹æ€§æµ‹è¯•")
    print("=" * 80)
    
    configs_dwt = MockConfigs(use_dwt_prompt=True, prompt_compression='balanced')
    model_dwt = Model(configs_dwt)
    model_dwt.eval()
    
    # æµ‹è¯•ä¸åŒbatch size
    batch_sizes = [1, 2, 4, 8]
    seq_len = 512
    pred_len = 96
    label_len = 48
    enc_in = 7
    
    print(f"\næµ‹è¯•ä¸åŒBatch Size:")
    for batch_size in batch_sizes:
        x_enc = torch.randn(batch_size, seq_len, enc_in)
        x_mark_enc = torch.zeros(batch_size, seq_len, 4)
        x_dec = torch.randn(batch_size, label_len + pred_len, enc_in)
        x_mark_dec = torch.zeros(batch_size, label_len + pred_len, 4)
        
        try:
            with torch.no_grad():
                output = model_dwt(x_enc, x_mark_enc, x_dec, x_mark_dec)
            assert output.shape == (batch_size, pred_len, enc_in)
            print(f"âœ… Batch={batch_size}: è¾“å‡ºå½¢çŠ¶ {output.shape}")
        except Exception as e:
            print(f"âŒ Batch={batch_size}: å¤±è´¥ - {e}")
            raise
    
    print("\nâœ… æ‰¹å¤„ç†å…¼å®¹æ€§æµ‹è¯•é€šè¿‡!")


def test_compression_levels():
    """æµ‹è¯•5: ä¸åŒå‹ç¼©çº§åˆ«å¯¹æ¯”"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•5: å‹ç¼©çº§åˆ«å¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    
    batch_size = 2
    seq_len = 512
    pred_len = 96
    label_len = 48
    enc_in = 7
    
    x_enc = torch.randn(batch_size, seq_len, enc_in)
    x_mark_enc = torch.zeros(batch_size, seq_len, 4)
    x_dec = torch.randn(batch_size, label_len + pred_len, enc_in)
    x_mark_dec = torch.zeros(batch_size, label_len + pred_len, 4)
    
    import time
    
    results = {}
    
    for compression in ['minimal', 'balanced', 'detailed']:
        print(f"\n[5.{['minimal', 'balanced', 'detailed'].index(compression)+1}] æµ‹è¯•{compression}æ¨¡å¼")
        
        configs = MockConfigs(use_dwt_prompt=True, prompt_compression=compression)
        model = Model(configs)
        model.eval()
        
        # é¢„çƒ­
        with torch.no_grad():
            _ = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        # æµ‹è¯•æ€§èƒ½
        start = time.time()
        with torch.no_grad():
            for _ in range(5):
                output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
        elapsed = (time.time() - start) / 5 * 1000  # ms
        
        # ç”Ÿæˆä¸€ä¸ªpromptçœ‹tokenæ•°
        x_enc_norm = model.normalize_layers(x_enc, 'norm')
        x_sample = x_enc_norm[0:1, :, :].permute(0, 2, 1)
        dwt_features = model.dwt_prompt_generator(x_sample)
        
        B, T, N = x_enc_norm.size()
        x_reshaped = x_enc_norm.permute(0, 2, 1).reshape(B * N, T, 1)
        min_val = torch.min(x_reshaped, dim=1)[0][0].item()
        max_val = torch.max(x_reshaped, dim=1)[0][0].item()
        median = torch.median(x_reshaped, dim=1).values[0].item()
        lags = model.calcute_lags(x_reshaped)[0].cpu().numpy()
        
        base_info = {
            'min': min_val, 'max': max_val, 'median': median, 'lags': lags,
            'description': model.description, 'seq_len': model.seq_len, 'pred_len': model.pred_len
        }
        prompt = model.dwt_prompt_generator.build_prompt_text(dwt_features, base_info)
        token_count = len(prompt.split())
        
        results[compression] = {
            'time': elapsed,
            'tokens': token_count,
            'output_shape': output.shape
        }
        
        print(f"  âœ… è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  âœ… æ¨ç†æ—¶é—´: {elapsed:.2f} ms")
        print(f"  âœ… Tokenæ•°: {token_count}")
    
    print("\n[5.4] æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print(f"{'æ¨¡å¼':<12} {'Tokenæ•°':<12} {'æ¨ç†æ—¶é—´(ms)':<15}")
    print("-" * 40)
    for mode, result in results.items():
        print(f"{mode:<12} {result['tokens']:<12} {result['time']:<15.2f}")
    
    print("\nâœ… å‹ç¼©çº§åˆ«å¯¹æ¯”æµ‹è¯•é€šè¿‡!")


def test_gpu_compatibility():
    """æµ‹è¯•6: GPUå…¼å®¹æ€§"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•6: GPUå…¼å®¹æ€§æµ‹è¯•")
    print("=" * 80)
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
        return
    
    print(f"âœ… æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
    
    configs_dwt = MockConfigs(use_dwt_prompt=True, prompt_compression='balanced')
    model_dwt = Model(configs_dwt).cuda()
    model_dwt.eval()
    
    batch_size = 2
    seq_len = 512
    pred_len = 96
    label_len = 48
    enc_in = 7
    
    x_enc = torch.randn(batch_size, seq_len, enc_in).cuda()
    x_mark_enc = torch.zeros(batch_size, seq_len, 4).cuda()
    x_dec = torch.randn(batch_size, label_len + pred_len, enc_in).cuda()
    x_mark_dec = torch.zeros(batch_size, label_len + pred_len, 4).cuda()
    
    print(f"\nè¾“å…¥è®¾å¤‡: {x_enc.device}")
    
    try:
        with torch.no_grad():
            output = model_dwt(x_enc, x_mark_enc, x_dec, x_mark_dec)
        print(f"âœ… GPU Forwardé€šè¿‡ï¼Œè¾“å‡ºè®¾å¤‡: {output.device}")
        print(f"âœ… è¾“å‡ºå½¢çŠ¶: {output.shape}")
        assert output.device.type == 'cuda'
    except Exception as e:
        print(f"âŒ GPUæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    print("\nâœ… GPUå…¼å®¹æ€§æµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    print("=" * 80)
    print("TimeLLM + DWT Prompt é›†æˆæµ‹è¯•å¥—ä»¶")
    print("=" * 80)
    print("\næœ¬æµ‹è¯•éªŒè¯DWT Promptç”Ÿæˆå™¨ä¸TimeLLMæ¨¡å‹çš„å®Œæ•´é›†æˆ")
    print("åŒ…æ‹¬ï¼šæ¨¡å‹åˆå§‹åŒ–ã€Forward Passã€Promptç”Ÿæˆã€æ‰¹å¤„ç†ã€å‹ç¼©çº§åˆ«ã€GPUå…¼å®¹æ€§")
    print("=" * 80)
    
    try:
        test_model_initialization()
        test_forward_pass()
        test_prompt_generation()
        test_batch_compatibility()
        test_compression_levels()
        test_gpu_compatibility()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡!")
        print("=" * 80)
        print("\nâœ… DWT Promptå·²æˆåŠŸé›†æˆåˆ°TimeLLMæ¨¡å‹")
        print("âœ… å¯ä»¥å¼€å§‹ä½¿ç”¨ --use_dwt_prompt True è¿›è¡Œè®­ç»ƒ")
        print("=" * 80)
        
    except Exception as e:
        print("\n" + "=" * 80)
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        print("=" * 80)
        import traceback
        traceback.print_exc()
        sys.exit(1)
