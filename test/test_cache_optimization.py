"""
ç¼“å­˜ä¼˜åŒ–æ€§èƒ½å¯¹æ¯”æµ‹è¯•
å¯¹æ¯”å¯ç”¨/ç¦ç”¨ç¼“å­˜çš„æ€§èƒ½å·®å¼‚
"""
import torch
import time
import sys
import os
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from layers.DWTPromptGenerator_performance_up import DWTPromptGenerator


def benchmark(generator, x_enc_list, warmup=5, n_runs=50):
    """
    åŸºå‡†æµ‹è¯•
    
    Args:
        generator: DWTç”Ÿæˆå™¨
        x_enc_list: è¾“å…¥æ•°æ®åˆ—è¡¨ï¼ˆæ¨¡æ‹Ÿå¤šä¸ªbatchï¼‰
        warmup: é¢„çƒ­æ¬¡æ•°
        n_runs: æµ‹è¯•æ¬¡æ•°
    
    Returns:
        dict: æ€§èƒ½ç»Ÿè®¡
    """
    device = x_enc_list[0].device
    
    # é¢„çƒ­
    for _ in range(warmup):
        for x_enc in x_enc_list:
            _ = generator(x_enc)
    
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    # æµ‹è¯•
    times = []
    for _ in range(n_runs):
        for x_enc in x_enc_list:
            if device.type == 'cuda':
                torch.cuda.synchronize()
                start = time.perf_counter()
            else:
                start = time.perf_counter()
            
            _ = generator(x_enc)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
                end = time.perf_counter()
            else:
                end = time.perf_counter()
            
            times.append((end - start) * 1000)
    
    return {
        'mean': np.mean(times),
        'std': np.std(times),
        'min': np.min(times),
        'max': np.max(times),
        'median': np.median(times)
    }


def test_cache_with_repetition():
    """æµ‹è¯•åœºæ™¯1: é«˜é‡å¤åº¦æ•°æ®ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒåœºæ™¯ï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯•åœºæ™¯1: é«˜é‡å¤åº¦æ•°æ®ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒåœºæ™¯ï¼‰")
    print("="*70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼š10ä¸ªä¸åŒçš„åºåˆ—ï¼Œæ¯ä¸ªé‡å¤å¤šæ¬¡
    unique_patterns = 10
    repeats = 5
    batch_size = 1
    n_vars = 7
    seq_len = 336
    
    # åˆ›å»º10ä¸ªç‹¬ç‰¹çš„æ¨¡å¼
    unique_data = [
        torch.randn(batch_size, n_vars, seq_len).to(device)
        for _ in range(unique_patterns)
    ]
    
    # æ¯ä¸ªæ¨¡å¼é‡å¤5æ¬¡ï¼Œæ¨¡æ‹Ÿè®­ç»ƒä¸­çš„ç›¸ä¼¼batch
    test_data = unique_data * repeats  # æ€»å…±50ä¸ªæ ·æœ¬
    
    print(f"\næ•°æ®é…ç½®:")
    print(f"  ç‹¬ç‰¹æ¨¡å¼æ•°: {unique_patterns}")
    print(f"  æ¯ä¸ªæ¨¡å¼é‡å¤: {repeats}æ¬¡")
    print(f"  æ€»æ ·æœ¬æ•°: {len(test_data)}")
    print(f"  å½¢çŠ¶: ({batch_size}, {n_vars}, {seq_len})")
    
    # æµ‹è¯•1: ç¦ç”¨ç¼“å­˜
    print("\n[1] ç¦ç”¨ç¼“å­˜æµ‹è¯•...")
    gen_no_cache = DWTPromptGenerator(enable_cache=False).to(device)
    stats_no_cache = benchmark(gen_no_cache, test_data, warmup=2, n_runs=3)
    print(f"  å¹³å‡æ—¶é—´: {stats_no_cache['mean']:.3f} Â± {stats_no_cache['std']:.3f} ms")
    
    # æµ‹è¯•2: å¯ç”¨ç¼“å­˜
    print("\n[2] å¯ç”¨ç¼“å­˜æµ‹è¯•...")
    gen_with_cache = DWTPromptGenerator(enable_cache=True, cache_size=100).to(device)
    stats_with_cache = benchmark(gen_with_cache, test_data, warmup=2, n_runs=3)
    cache_stats = gen_with_cache.get_cache_stats()
    
    print(f"  å¹³å‡æ—¶é—´: {stats_with_cache['mean']:.3f} Â± {stats_with_cache['std']:.3f} ms")
    print(f"\n  ç¼“å­˜ç»Ÿè®¡:")
    print(f"    å‘½ä¸­æ¬¡æ•°: {cache_stats['hits']}")
    print(f"    æœªå‘½ä¸­æ¬¡æ•°: {cache_stats['misses']}")
    print(f"    å‘½ä¸­ç‡: {cache_stats['hit_rate']:.1f}%")
    print(f"    ç¼“å­˜å¤§å°: {cache_stats['cache_size']}/{cache_stats['cache_limit']}")
    
    # è®¡ç®—åŠ é€Ÿæ¯”
    speedup = stats_no_cache['mean'] / stats_with_cache['mean']
    improvement = (stats_no_cache['mean'] - stats_with_cache['mean']) / stats_no_cache['mean'] * 100
    
    print(f"\n  [æ€§èƒ½æå‡]")
    print(f"    åŠ é€Ÿæ¯”: {speedup:.2f}x")
    print(f"    æ€§èƒ½æå‡: {improvement:.1f}%")
    print(f"    æ—¶é—´èŠ‚çœ: {stats_no_cache['mean'] - stats_with_cache['mean']:.3f} ms/call")
    
    return {
        'scenario': 'high_repetition',
        'no_cache': stats_no_cache,
        'with_cache': stats_with_cache,
        'cache_stats': cache_stats,
        'speedup': speedup,
        'improvement': improvement
    }


def test_cache_with_unique_data():
    """æµ‹è¯•åœºæ™¯2: å®Œå…¨ç‹¬ç‰¹æ•°æ®ï¼ˆæœ€åæƒ…å†µï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯•åœºæ™¯2: å®Œå…¨ç‹¬ç‰¹æ•°æ®ï¼ˆç¼“å­˜æœ€åæƒ…å†µï¼‰")
    print("="*70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    batch_size = 1
    n_vars = 7
    seq_len = 336
    n_samples = 50
    
    # æ¯ä¸ªæ ·æœ¬éƒ½å®Œå…¨ä¸åŒ
    test_data = [
        torch.randn(batch_size, n_vars, seq_len).to(device)
        for _ in range(n_samples)
    ]
    
    print(f"\næ•°æ®é…ç½®:")
    print(f"  æ€»æ ·æœ¬æ•°: {n_samples} (å…¨éƒ¨ç‹¬ç‰¹)")
    print(f"  å½¢çŠ¶: ({batch_size}, {n_vars}, {seq_len})")
    
    # æµ‹è¯•1: ç¦ç”¨ç¼“å­˜
    print("\n[1] ç¦ç”¨ç¼“å­˜æµ‹è¯•...")
    gen_no_cache = DWTPromptGenerator(enable_cache=False).to(device)
    stats_no_cache = benchmark(gen_no_cache, test_data, warmup=2, n_runs=3)
    print(f"  å¹³å‡æ—¶é—´: {stats_no_cache['mean']:.3f} Â± {stats_no_cache['std']:.3f} ms")
    
    # æµ‹è¯•2: å¯ç”¨ç¼“å­˜
    print("\n[2] å¯ç”¨ç¼“å­˜æµ‹è¯•...")
    gen_with_cache = DWTPromptGenerator(enable_cache=True, cache_size=100).to(device)
    stats_with_cache = benchmark(gen_with_cache, test_data, warmup=2, n_runs=3)
    cache_stats = gen_with_cache.get_cache_stats()
    
    print(f"  å¹³å‡æ—¶é—´: {stats_with_cache['mean']:.3f} Â± {stats_with_cache['std']:.3f} ms")
    print(f"\n  ç¼“å­˜ç»Ÿè®¡:")
    print(f"    å‘½ä¸­æ¬¡æ•°: {cache_stats['hits']}")
    print(f"    æœªå‘½ä¸­æ¬¡æ•°: {cache_stats['misses']}")
    print(f"    å‘½ä¸­ç‡: {cache_stats['hit_rate']:.1f}%")
    print(f"    ç¼“å­˜å¼€é”€: {stats_with_cache['mean'] - stats_no_cache['mean']:.3f} ms")
    
    overhead = (stats_with_cache['mean'] - stats_no_cache['mean']) / stats_no_cache['mean'] * 100
    print(f"    ç›¸å¯¹å¼€é”€: {overhead:.2f}%")
    
    return {
        'scenario': 'unique_data',
        'no_cache': stats_no_cache,
        'with_cache': stats_with_cache,
        'cache_stats': cache_stats,
        'overhead': overhead
    }


def test_cache_with_mixed_data():
    """æµ‹è¯•åœºæ™¯3: æ··åˆæ•°æ®ï¼ˆçœŸå®åœºæ™¯ï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯•åœºæ™¯3: æ··åˆæ•°æ®ï¼ˆçœŸå®è®­ç»ƒåœºæ™¯æ¨¡æ‹Ÿï¼‰")
    print("="*70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    batch_size = 1
    n_vars = 7
    seq_len = 336
    
    # åˆ›å»º20ä¸ªåŸºç¡€æ¨¡å¼
    base_patterns = [
        torch.randn(batch_size, n_vars, seq_len).to(device)
        for _ in range(20)
    ]
    
    # 70%é‡å¤ï¼Œ30%æ–°æ•°æ®
    test_data = []
    for _ in range(35):  # 70% é‡å¤
        test_data.append(base_patterns[np.random.randint(0, 20)])
    for _ in range(15):  # 30% æ–°æ•°æ®
        test_data.append(torch.randn(batch_size, n_vars, seq_len).to(device))
    
    print(f"\næ•°æ®é…ç½®:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(test_data)}")
    print(f"  é‡å¤æ•°æ®: ~70%")
    print(f"  ç‹¬ç‰¹æ•°æ®: ~30%")
    print(f"  å½¢çŠ¶: ({batch_size}, {n_vars}, {seq_len})")
    
    # æµ‹è¯•1: ç¦ç”¨ç¼“å­˜
    print("\n[1] ç¦ç”¨ç¼“å­˜æµ‹è¯•...")
    gen_no_cache = DWTPromptGenerator(enable_cache=False).to(device)
    stats_no_cache = benchmark(gen_no_cache, test_data, warmup=2, n_runs=3)
    print(f"  å¹³å‡æ—¶é—´: {stats_no_cache['mean']:.3f} Â± {stats_no_cache['std']:.3f} ms")
    
    # æµ‹è¯•2: å¯ç”¨ç¼“å­˜
    print("\n[2] å¯ç”¨ç¼“å­˜æµ‹è¯•...")
    gen_with_cache = DWTPromptGenerator(enable_cache=True, cache_size=100).to(device)
    stats_with_cache = benchmark(gen_with_cache, test_data, warmup=2, n_runs=3)
    cache_stats = gen_with_cache.get_cache_stats()
    
    print(f"  å¹³å‡æ—¶é—´: {stats_with_cache['mean']:.3f} Â± {stats_with_cache['std']:.3f} ms")
    print(f"\n  ç¼“å­˜ç»Ÿè®¡:")
    print(f"    å‘½ä¸­æ¬¡æ•°: {cache_stats['hits']}")
    print(f"    æœªå‘½ä¸­æ¬¡æ•°: {cache_stats['misses']}")
    print(f"    å‘½ä¸­ç‡: {cache_stats['hit_rate']:.1f}%")
    print(f"    ç¼“å­˜å¤§å°: {cache_stats['cache_size']}/{cache_stats['cache_limit']}")
    
    speedup = stats_no_cache['mean'] / stats_with_cache['mean']
    improvement = (stats_no_cache['mean'] - stats_with_cache['mean']) / stats_no_cache['mean'] * 100
    
    print(f"\n  [æ€§èƒ½æå‡]")
    print(f"    åŠ é€Ÿæ¯”: {speedup:.2f}x")
    print(f"    æ€§èƒ½æå‡: {improvement:.1f}%")
    
    return {
        'scenario': 'mixed_data',
        'no_cache': stats_no_cache,
        'with_cache': stats_with_cache,
        'cache_stats': cache_stats,
        'speedup': speedup,
        'improvement': improvement
    }


def test_cache_size_impact():
    """æµ‹è¯•åœºæ™¯4: ç¼“å­˜å¤§å°å½±å“"""
    print("\n" + "="*70)
    print("æµ‹è¯•åœºæ™¯4: ç¼“å­˜å¤§å°å¯¹æ€§èƒ½çš„å½±å“")
    print("="*70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»º100ä¸ªä¸åŒæ¨¡å¼ï¼Œæ¯ä¸ªé‡å¤2æ¬¡
    unique_patterns = 100
    repeats = 2
    batch_size = 1
    n_vars = 7
    seq_len = 336
    
    unique_data = [
        torch.randn(batch_size, n_vars, seq_len).to(device)
        for _ in range(unique_patterns)
    ]
    test_data = unique_data * repeats
    
    print(f"\næ•°æ®é…ç½®: {unique_patterns}ä¸ªæ¨¡å¼ Ã— {repeats}æ¬¡ = {len(test_data)}ä¸ªæ ·æœ¬")
    
    cache_sizes = [10, 50, 100, 200, 500]
    results = []
    
    print(f"\n{'ç¼“å­˜å¤§å°':<10} | {'å¹³å‡æ—¶é—´(ms)':<12} | {'å‘½ä¸­ç‡':<10} | {'åŠ é€Ÿæ¯”':<8}")
    print("-" * 50)
    
    for cache_size in cache_sizes:
        gen = DWTPromptGenerator(enable_cache=True, cache_size=cache_size).to(device)
        stats = benchmark(gen, test_data, warmup=1, n_runs=2)
        cache_stats = gen.get_cache_stats()
        
        # å‚è€ƒï¼šæ— ç¼“å­˜çš„æ—¶é—´ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œæ—¶è®¡ç®—ï¼‰
        if len(results) == 0:
            gen_ref = DWTPromptGenerator(enable_cache=False).to(device)
            ref_stats = benchmark(gen_ref, test_data, warmup=1, n_runs=2)
            ref_time = ref_stats['mean']
        
        speedup = ref_time / stats['mean']
        
        print(f"{cache_size:<10} | {stats['mean']:<12.3f} | {cache_stats['hit_rate']:<9.1f}% | {speedup:<8.2f}x")
        
        results.append({
            'cache_size': cache_size,
            'time': stats['mean'],
            'hit_rate': cache_stats['hit_rate'],
            'speedup': speedup
        })
    
    return results


def main():
    print("="*70)
    print("DWT Prompt Generator - ç¼“å­˜ä¼˜åŒ–æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    if device.type == 'cuda':
        print(f"GPUå‹å·: {torch.cuda.get_device_name(0)}")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    result1 = test_cache_with_repetition()
    result2 = test_cache_with_unique_data()
    result3 = test_cache_with_mixed_data()
    result4 = test_cache_size_impact()
    
    # æ‰“å°æ±‡æ€»
    print("\n" + "="*70)
    print("æ€§èƒ½ä¼˜åŒ–æ±‡æ€»")
    print("="*70)
    
    print(f"\nåœºæ™¯1 - é«˜é‡å¤åº¦æ•°æ® (è®­ç»ƒåœºæ™¯):")
    print(f"  åŠ é€Ÿæ¯”: {result1['speedup']:.2f}x")
    print(f"  æ€§èƒ½æå‡: {result1['improvement']:.1f}%")
    print(f"  ç¼“å­˜å‘½ä¸­ç‡: {result1['cache_stats']['hit_rate']:.1f}%")
    
    print(f"\nåœºæ™¯2 - å®Œå…¨ç‹¬ç‰¹æ•°æ® (æœ€åæƒ…å†µ):")
    print(f"  ç¼“å­˜å¼€é”€: {result2['overhead']:.2f}%")
    print(f"  ç»“è®º: ç¼“å­˜å¼€é”€å¯å¿½ç•¥")
    
    print(f"\nåœºæ™¯3 - æ··åˆæ•°æ® (çœŸå®åœºæ™¯):")
    print(f"  åŠ é€Ÿæ¯”: {result3['speedup']:.2f}x")
    print(f"  æ€§èƒ½æå‡: {result3['improvement']:.1f}%")
    print(f"  ç¼“å­˜å‘½ä¸­ç‡: {result3['cache_stats']['hit_rate']:.1f}%")
    
    print(f"\nåœºæ™¯4 - ç¼“å­˜å¤§å°å½±å“:")
    print(f"  æ¨èç¼“å­˜å¤§å°: 100-200")
    print(f"  åŸå› : å¹³è¡¡å†…å­˜å ç”¨å’Œå‘½ä¸­ç‡")
    
    print("\n" + "="*70)
    print("âœ… æµ‹è¯•å®Œæˆï¼ç¼“å­˜ä¼˜åŒ–æ˜¾è‘—æå‡æ€§èƒ½")
    print("="*70)
    
    print(f"\nğŸ’¡ å…³é”®å‘ç°:")
    print(f"  1. è®­ç»ƒåœºæ™¯(é«˜é‡å¤)åŠ é€Ÿ: {result1['speedup']:.1f}x")
    print(f"  2. çœŸå®æ··åˆåœºæ™¯åŠ é€Ÿ: {result3['speedup']:.1f}x")
    print(f"  3. æœ€åæƒ…å†µå¼€é”€: <5%")
    print(f"  4. æ¨èé…ç½®: enable_cache=True, cache_size=100")


if __name__ == '__main__':
    main()
