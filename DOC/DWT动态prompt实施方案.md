# æ–¹æ¡ˆ B æ·±åº¦åˆ†æä¸ä¼˜åŒ–

## ä¸€ã€æ–¹æ¡ˆ B æ ¸å¿ƒè®¾è®¡è§£æ„

### 1.1 å››å¤§ç‰¹å¾ç»„çš„è®¾è®¡é€»è¾‘

```python
æ–¹æ¡ˆBç‰¹å¾æ¶æ„
â”‚
â”œâ”€ ç‰¹å¾ç»„1: èƒ½é‡åˆ†å¸ƒ â†’ åºåˆ—ç±»å‹åˆ†ç±»
â”‚   â”œâ”€ è¾“å…¥: 4ä¸ªé¢‘æ®µçš„èƒ½é‡å æ¯” [cA, cD3, cD2, cD1]
â”‚   â”œâ”€ è¾“å‡º: pattern_type (4ç±»è¯­ä¹‰æ ‡ç­¾)
â”‚   â””â”€ ç›®çš„: å‘Šè¯‰LLM"è¿™æ˜¯ä»€ä¹ˆæ ·çš„åºåˆ—"
â”‚
â”œâ”€ ç‰¹å¾ç»„2: å¤šå°ºåº¦è¶‹åŠ¿ â†’ è¶‹åŠ¿ä¸€è‡´æ€§æè¿°
â”‚   â”œâ”€ è¾“å…¥: 4ä¸ªé¢‘æ®µçš„è¶‹åŠ¿æ–¹å‘ [+120, -5, +15, -2]
â”‚   â”œâ”€ è¾“å‡º: trend_consistency (è‡ªç„¶è¯­è¨€)
â”‚   â””â”€ ç›®çš„: æ›¿ä»£åŸç‰ˆå•ä¸€çš„upward/downward
â”‚
â”œâ”€ ç‰¹å¾ç»„3: æ³¢åŠ¨æ€§å±‚çº§ â†’ ç¨³å®šæ€§è¯„ä¼°
â”‚   â”œâ”€ è¾“å…¥: 4ä¸ªé¢‘æ®µçš„æ ‡å‡†å·® [0.1, 0.3, 0.8, 1.2]
â”‚   â”œâ”€ è¾“å‡º: stability_desc (è¯­ä¹‰æ ‡ç­¾)
â”‚   â””â”€ ç›®çš„: é‡åŒ–å™ªå£°vsä¿¡å·çš„å…³ç³»
â”‚
â””â”€ ç‰¹å¾ç»„4: ä¿¡å·å¤æ‚åº¦ â†’ é¢„æµ‹éš¾åº¦æç¤º
    â”œâ”€ è¾“å…¥: é«˜é¢‘èƒ½é‡å æ¯”
    â”œâ”€ è¾“å‡º: difficulty (3çº§åˆ†ç±»)
    â””â”€ ç›®çš„: å¸®åŠ©LLMè°ƒæ•´é¢„æµ‹ç½®ä¿¡åº¦
```

---

## äºŒã€æ–¹æ¡ˆ B çš„æ½œåœ¨é—®é¢˜ä¸ä¼˜åŒ–

### 2.1 é—®é¢˜1: é˜ˆå€¼ç¡¬ç¼–ç 

#### å½“å‰è®¾è®¡
```python
# ç‰¹å¾ç»„1: èƒ½é‡åˆ†å¸ƒé˜ˆå€¼
if cA_ratio > 0.8:           # ğŸš¨ ç¡¬ç¼–ç 
    pattern_type = "smooth trend-dominated"
elif cA_ratio > 0.6:         # ğŸš¨ ç¡¬ç¼–ç 
    pattern_type = "trend with moderate fluctuations"
elif energy_ratio[1] > 0.3:  # ğŸš¨ ç¡¬ç¼–ç 
    pattern_type = "strong periodic pattern"

# ç‰¹å¾ç»„3: æ³¢åŠ¨æ€§é˜ˆå€¼
if high_freq_vol / low_freq_vol > 5:  # ğŸš¨ ç¡¬ç¼–ç 
    stability_desc = "stable trend with high short-term volatility"
```

#### é—®é¢˜åˆ†æ
| é—®é¢˜ | å½±å“ | ä¸¥é‡ç¨‹åº¦ |
|------|------|---------|
| **æ•°æ®é›†ä¾èµ–** | ETTæ•°æ®é›†çš„é˜ˆå€¼å¯èƒ½ä¸é€‚ç”¨äºWeather/Electricity | ğŸ”´ é«˜ |
| **ç¼ºä¹ç†è®ºä¾æ®** | 0.8, 0.6, 5ç­‰æ•°å€¼ç¼ºä¹ä¿¡å·å¤„ç†ç†è®ºæ”¯æ’‘ | ğŸŸ¡ ä¸­ |
| **éš¾ä»¥è°ƒä¼˜** | éœ€è¦å¤§é‡å®éªŒæ‰èƒ½æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼ | ğŸŸ¡ ä¸­ |

#### âœ… ä¼˜åŒ–æ–¹æ¡ˆ1: è‡ªé€‚åº”é˜ˆå€¼ï¼ˆåŸºäºç™¾åˆ†ä½æ•°ï¼‰

```python
def calculate_adaptive_thresholds(self, x_enc, percentiles=[25, 50, 75]):
    """åœ¨éªŒè¯é›†ä¸Šç»Ÿè®¡é˜ˆå€¼åˆ†å¸ƒ"""
    # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„èƒ½é‡åˆ†å¸ƒ
    all_cA_ratios = []
    all_vol_ratios = []
    
    for batch in val_loader:
        coeffs = ptwt.wavedec(batch, 'db4', level=3)
        energies = [torch.sum(c**2) for c in coeffs]
        cA_ratio = energies[0] / sum(energies)
        all_cA_ratios.append(cA_ratio.item())
        
        vols = [torch.std(c) for c in coeffs]
        all_vol_ratios.append((vols[-1] / vols[0]).item())
    
    # ä½¿ç”¨ç™¾åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
    thresholds = {
        'cA_high': np.percentile(all_cA_ratios, 75),    # 75% â†’ smooth
        'cA_mid': np.percentile(all_cA_ratios, 50),     # 50% â†’ moderate
        'vol_high': np.percentile(all_vol_ratios, 75),  # 75% â†’ high volatility
    }
    
    return thresholds

def calculate_wavelet_features_B_adaptive(self, x_enc, thresholds):
    """ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼çš„ç‰ˆæœ¬"""
    # ... DWTåˆ†è§£ ...
    
    cA_ratio = energy_ratio[0].mean().item()
    if cA_ratio > thresholds['cA_high']:
        pattern_type = "smooth trend-dominated"
    elif cA_ratio > thresholds['cA_mid']:
        pattern_type = "trend with moderate fluctuations"
    # ...
```

**ä¼˜ç‚¹**:
- âœ… æ•°æ®é›†è‡ªé€‚åº”
- âœ… ç†è®ºä¾æ®æ›´å……åˆ†ï¼ˆç›¸å¯¹åˆ†å¸ƒï¼‰
- âœ… å‡å°‘æ‰‹åŠ¨è°ƒå‚

**ç¼ºç‚¹**:
- âŒ éœ€è¦é¢å¤–çš„ç»Ÿè®¡æ­¥éª¤
- âŒ å¢åŠ åˆå§‹åŒ–å¼€é”€

#### âœ… ä¼˜åŒ–æ–¹æ¡ˆ2: è¿ç»­è¯­ä¹‰æ˜ å°„ï¼ˆé¿å…ç¡¬åˆ†ç±»ï¼‰

```python
def continuous_semantic_mapping(self, cA_ratio):
    """ç”¨è¿ç»­å‡½æ•°æ›¿ä»£ç¡¬é˜ˆå€¼"""
    # ä½¿ç”¨ sigmoid å¹³æ»‘è¿‡æ¸¡
    smoothness_score = 1 / (1 + np.exp(-10 * (cA_ratio - 0.7)))
    
    if smoothness_score > 0.9:
        return "extremely smooth trend-dominated"
    elif smoothness_score > 0.7:
        return "smooth trend-dominated"
    elif smoothness_score > 0.5:
        return "trend with moderate fluctuations"
    elif smoothness_score > 0.3:
        return "fluctuation-dominated with underlying trend"
    else:
        return "complex multi-scale dynamics"
```

**ä¼˜ç‚¹**:
- âœ… å¹³æ»‘è¿‡æ¸¡ï¼Œé¿å…è¾¹ç•Œæ•ˆåº”
- âœ… æ›´ç»†ç²’åº¦çš„æè¿°

**ç¼ºç‚¹**:
- âŒ å¢åŠ äº†ç±»åˆ«æ•°é‡ï¼ˆå¯èƒ½å¢åŠ tokenï¼‰

---

### 2.2 é—®é¢˜2: ç‰¹å¾å†—ä½™ä¸ä¿¡æ¯æŸå¤±

#### å½“å‰è®¾è®¡çš„å†—ä½™

```python
# å†—ä½™1: è¶‹åŠ¿ä¿¡æ¯é‡å¤
åŸç‰ˆ: trend = 'upward' if trends > 0 else 'downward'
æ–¹æ¡ˆB: trend_consistency = "consistently upward across all scales"
# é—®é¢˜: å¦‚æœä¸€è‡´ï¼Œæ–¹æ¡ˆBå®é™…æ²¡æœ‰æ¯”åŸç‰ˆå¤šæä¾›ä¿¡æ¯

# å†—ä½™2: èƒ½é‡åˆ†å¸ƒä¸pattern_type
energy_ratio = [85%, 8%, 5%, 2%]
pattern_type = "smooth trend-dominated"
# é—®é¢˜: pattern_type å·²ç»éšå«äº†èƒ½é‡åˆ†å¸ƒä¿¡æ¯
```

#### âœ… ä¼˜åŒ–æ–¹æ¡ˆ3: ä¿¡æ¯ç†µæœ€å¤§åŒ–è®¾è®¡

```python
def calculate_wavelet_features_B_optimized(self, x_enc):
    """ä¼˜åŒ–ç‰ˆæœ¬: æœ€å¤§åŒ–ä¿¡æ¯ç†µï¼Œæœ€å°åŒ–å†—ä½™"""
    
    coeffs = ptwt.wavedec(x_enc, 'db4', level=3)
    energies = [torch.sum(c**2, dim=-1) for c in coeffs]
    total_energy = sum(energies)
    energy_ratio = [e / total_energy for e in energies]
    
    # === ç‰¹å¾1: é¢‘åŸŸç‰¹å¾ï¼ˆæ–°è®¾è®¡ï¼‰===
    # ä¸å†å•ç‹¬è¾“å‡ºpattern_typeï¼Œè€Œæ˜¯ç»“åˆèƒ½é‡+ç†µ
    energy_entropy = -sum([p * torch.log(p + 1e-10) for p in energy_ratio]).mean().item()
    
    if energy_entropy < 0.5:
        freq_desc = "single-scale dominant"  # èƒ½é‡é›†ä¸­
    elif energy_entropy < 1.0:
        freq_desc = "dual-scale pattern"
    else:
        freq_desc = "multi-scale complex"
    
    # é™„åŠ ä¸»å¯¼é¢‘æ®µä¿¡æ¯ï¼ˆä»…å½“æœ‰æ˜æ˜¾ä¸»å¯¼æ—¶ï¼‰
    dominant_idx = torch.argmax(torch.stack(energy_ratio, dim=0), dim=0).mode().values.item()
    dominant_energy = energy_ratio[dominant_idx].mean().item()
    
    if dominant_energy > 0.7:  # åªæœ‰æ˜æ˜¾ä¸»å¯¼æ—¶æ‰è¡¥å……
        band_names = ['trend', 'seasonal', 'fluctuation', 'noise']
        freq_desc += f" ({band_names[dominant_idx]})"
    
    # === ç‰¹å¾2: è¶‹åŠ¿å¤æ‚åº¦ï¼ˆå¢å¼ºç‰ˆï¼‰===
    trends = [(c[..., 1:] - c[..., :-1]).sum(dim=-1).mean().item() for c in coeffs]
    trends_normalized = [t / (torch.std(c).mean().item() + 1e-6) for c, t in zip(coeffs, trends)]
    
    # è®¡ç®—è¶‹åŠ¿ä¸€è‡´æ€§åˆ†æ•°
    trends_sign = [1 if t > 0 else -1 for t in trends]
    consistency_score = sum([1 for s in trends_sign if s == trends_sign[0]]) / len(trends_sign)
    
    if consistency_score == 1.0:
        trend_desc = f"consistent {'upward' if trends_sign[0] > 0 else 'downward'}"
    elif consistency_score >= 0.75:
        # æ‰¾å‡ºä¸ä¸€è‡´çš„é¢‘æ®µ
        inconsistent = [i for i, s in enumerate(trends_sign) if s != trends_sign[0]]
        band_names = ['long-term', 'seasonal', 'medium-term', 'short-term']
        trend_desc = f"mostly {'upward' if trends_sign[0] > 0 else 'downward'}, except {band_names[inconsistent[0]]}"
    else:
        # å¤šå°ºåº¦æ··åˆ
        up_count = sum([1 for s in trends_sign if s > 0])
        trend_desc = f"mixed ({up_count}/4 scales upward)"
    
    # === ç‰¹å¾3: ä¿¡å™ªæ¯”ï¼ˆæ›¿ä»£æ³¢åŠ¨æ€§ï¼‰===
    signal = torch.std(coeffs[0]).mean().item()  # cA3 æ ‡å‡†å·®ä½œä¸ºä¿¡å·
    noise = torch.std(coeffs[-1]).mean().item()  # cD1 æ ‡å‡†å·®ä½œä¸ºå™ªå£°
    snr_db = 10 * np.log10((signal ** 2) / (noise ** 2 + 1e-10))
    
    if snr_db > 20:
        quality_desc = "high SNR (clean)"
    elif snr_db > 10:
        quality_desc = "moderate SNR"
    else:
        quality_desc = "low SNR (noisy)"
    
    # === ç‰¹å¾4: é¢„æµ‹éš¾åº¦ï¼ˆåŸºäºå¤šå› ç´ ï¼‰===
    difficulty_score = (
        (1 - consistency_score) * 30 +     # è¶‹åŠ¿ä¸ä¸€è‡´ â†’ å›°éš¾
        energy_entropy * 20 +              # èƒ½é‡åˆ†æ•£ â†’ å›°éš¾
        max(0, 15 - snr_db) * 2            # ä½SNR â†’ å›°éš¾
    )
    
    if difficulty_score < 15:
        difficulty = "low"
    elif difficulty_score < 30:
        difficulty = "moderate"
    else:
        difficulty = "high"
    
    return {
        'freq_pattern': freq_desc,
        'trend': trend_desc,
        'signal_quality': quality_desc,
        'difficulty': difficulty,
        'snr_db': snr_db,
        'energy_entropy': energy_entropy
    }
```

**ä¼˜åŒ–äº®ç‚¹**:
1. **ç†µå€¼å¼•å…¥**: ç”¨ä¿¡æ¯ç†µé‡åŒ–èƒ½é‡åˆ†å¸ƒå¤æ‚åº¦
2. **SNRæ›¿ä»£æ³¢åŠ¨æ€§**: ä¿¡å™ªæ¯”æ˜¯ä¿¡å·å¤„ç†çš„æ ‡å‡†æŒ‡æ ‡ï¼Œæ›´æœ‰ç†è®ºä¾æ®
3. **è¶‹åŠ¿ä¸€è‡´æ€§é‡åŒ–**: 0.75 é˜ˆå€¼æ›´ç²¾ç¡®æè¿°"å¤§éƒ¨åˆ†ä¸€è‡´"
4. **å¤šå› ç´ éš¾åº¦**: ç»¼åˆ3ä¸ªç»´åº¦è®¡ç®—é¢„æµ‹éš¾åº¦

---

### 2.3 é—®é¢˜3: è¯­ä¹‰æè¿°çš„LLMç†è§£èƒ½åŠ›

#### å½“å‰è®¾è®¡çš„è¯­ä¹‰å¤æ‚åº¦

```python
# ç¤ºä¾‹1: è¿‡äºæŠ€æœ¯åŒ–
"stable trend with high short-term volatility"
# LLMå¯èƒ½ç†è§£: "ç¨³å®š" vs "é«˜æ³¢åŠ¨" çŸ›ç›¾ï¼Ÿ

# ç¤ºä¾‹2: è¿‡äºæŠ½è±¡
"complex multi-scale dynamics"
# LLMå¯èƒ½ç†è§£: è¿™å¯¹é¢„æµ‹æœ‰ä»€ä¹ˆå…·ä½“å½±å“ï¼Ÿ
```

#### âœ… ä¼˜åŒ–æ–¹æ¡ˆ4: åŠŸèƒ½æ€§æè¿°ï¼ˆå‘Šè¯‰LLM"æ€ä¹ˆåš"è€Œé"æ˜¯ä»€ä¹ˆ"ï¼‰

```python
def functional_semantic_mapping(self, features):
    """å°†ç‰¹å¾è½¬åŒ–ä¸ºåŠŸèƒ½æ€§æŒ‡ä»¤"""
    
    # åŸç‰ˆè¯­ä¹‰æè¿°
    pattern_type = "smooth trend-dominated"
    
    # åŠŸèƒ½æ€§æè¿°ï¼ˆæ›´actionableï¼‰
    functional_desc = {
        "smooth trend-dominated": 
            "Focus on extrapolating the main trend, noise can be ignored",
        
        "strong periodic pattern":
            "Identify and extend the periodic cycles, pay attention to phase",
        
        "complex multi-scale dynamics":
            "Balance between multiple time scales, high uncertainty expected",
        
        "high short-term volatility":
            "Main trend is reliable, but short-term fluctuations are unpredictable"
    }
    
    return functional_desc.get(pattern_type, pattern_type)
```

**ç¤ºä¾‹å¯¹æ¯”**:

```
æè¿°æ€§ (åŸæ–¹æ¡ˆB):
"Pattern: smooth trend-dominated
 Trend: consistently upward
 Stability: high short-term volatility"

åŠŸèƒ½æ€§ (ä¼˜åŒ–ç‰ˆ):
"Pattern: Focus on extrapolating the main upward trend
 Note: Short-term fluctuations are unpredictable, prioritize long-term direction"
```

**ä¼˜ç‚¹**:
- âœ… ç»™LLMæ˜ç¡®çš„è¡ŒåŠ¨æŒ‡å¼•
- âœ… å‡å°‘è¯­ä¹‰æ­§ä¹‰
- âœ… æ›´æ¥è¿‘instruction-followingèŒƒå¼

**ç¼ºç‚¹**:
- âŒ Tokenæ•°é‡å¯èƒ½å¢åŠ 
- âŒ éœ€è¦ç²¾å¿ƒè®¾è®¡æŒ‡ä»¤æ¨¡æ¿

---

### 2.4 é—®é¢˜4: Tokenæ•ˆç‡ vs ä¿¡æ¯é‡æƒè¡¡

#### å½“å‰æ–¹æ¡ˆBçš„Tokenåˆ†æ

```python
åŸç‰ˆ (69 tokens):
"min=-1.2, max=2.5, median=0.3, trend=upward, lags=[24,48,96,168,336]"

æ–¹æ¡ˆB (85 tokens, +23%):
"Range: [-1.2, 2.5], median=0.3
Pattern: smooth trend-dominated
Trend: consistently upward across all scales
Stability: smooth and predictable
Difficulty: low (clean signal)
Periodicities: [24, 48, 96]"
```

#### âœ… ä¼˜åŒ–æ–¹æ¡ˆ5: åˆ†å±‚è¯¦ç»†åº¦ï¼ˆæŒ‰é‡è¦æ€§å‹ç¼©ï¼‰

```python
def build_prompt_B_compressed(self, x_enc, ...):
    """å‹ç¼©ç‰ˆæ–¹æ¡ˆB: ä¿ç•™æ ¸å¿ƒä¿¡æ¯ï¼Œç®€åŒ–æ¬¡è¦æè¿°"""
    
    wf = self.calculate_wavelet_features_B_optimized(x_enc)
    
    # æ ¸å¿ƒä¿¡æ¯ï¼ˆå¿…é¡»ä¿ç•™ï¼‰
    core_info = f"Range:[{min_val:.1f},{max_val:.1f}]@{median:.1f}"
    
    # å°æ³¢ç‰¹å¾ï¼ˆç²¾ç®€è¡¨è¾¾ï¼‰
    # ä½¿ç”¨ç¼©å†™ + å…³é”®è¯
    wavelet_info = f"Pattern:{wf['freq_pattern']}, Trend:{wf['trend']}, SNR:{wf['signal_quality']}"
    
    # é¢„æµ‹éš¾åº¦ï¼ˆå¦‚æœæ˜¯moderateåˆ™çœç•¥ï¼ŒåªæŠ¥å‘Šæç«¯æƒ…å†µï¼‰
    difficulty_info = f", Difficulty:{wf['difficulty']}" if wf['difficulty'] != 'moderate' else ""
    
    # å‘¨æœŸæ€§ï¼ˆåªä¿ç•™å‰2ä¸ªæœ€å¼ºå‘¨æœŸï¼‰
    lags = self.calcute_lags(x_enc)[:2]
    
    prompt = f"""
<|start_prompt|>
{self.description}
Forecast {self.pred_len} from {self.seq_len}: {core_info}
{wavelet_info}{difficulty_info}
Cycles: {lags.tolist()}
<|<end_prompt>|>
"""
    # é¢„è®¡: ~72 tokens (+4%ç›¸æ¯”åŸç‰ˆ, -15%ç›¸æ¯”æ–¹æ¡ˆBæ ‡å‡†ç‰ˆ)
    
    return prompt
```

**Tokenä¼˜åŒ–ç­–ç•¥**:
| æŠ€å·§ | ç¤ºä¾‹ | èŠ‚çœToken |
|------|------|----------|
| ç¼©å†™å•ä½ | "Range:[-1.2,2.5]" vs "min=-1.2, max=2.5" | -3 |
| çœç•¥å†—ä½™ | åªæŠ¥å‘Šémoderateéš¾åº¦ | -5 |
| ç²¾ç®€å‘¨æœŸ | Top-2 vs Top-5 lags | -6 |
| åˆå¹¶æè¿° | "SNR:high" vs "Stability: smooth and predictable pattern" | -4 |

---

## ä¸‰ã€æ–¹æ¡ˆBæœ€ç»ˆä¼˜åŒ–ç‰ˆè®¾è®¡

### 3.1 æ¨èé…ç½®

```python
class WaveletPromptGeneratorB:
    """æ–¹æ¡ˆBä¼˜åŒ–ç‰ˆå®ç°"""
    
    def __init__(self, use_adaptive_thresholds=True, 
                 use_functional_desc=False,
                 compression_level='balanced'):
        """
        Args:
            use_adaptive_thresholds: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼ï¼ˆæ¨èTrueï¼‰
            use_functional_desc: æ˜¯å¦ä½¿ç”¨åŠŸèƒ½æ€§æè¿°ï¼ˆå®éªŒæ€§ï¼‰
            compression_level: 'minimal' | 'balanced' | 'detailed'
        """
        self.use_adaptive = use_adaptive_thresholds
        self.functional = use_functional_desc
        self.compression = compression_level
        
        # é»˜è®¤é˜ˆå€¼ï¼ˆå¦‚æœä¸ç”¨è‡ªé€‚åº”ï¼‰
        self.thresholds = {
            'cA_high': 0.75,  # é™ä½from 0.8ï¼Œæ›´å®½å®¹
            'cA_mid': 0.55,   # é™ä½from 0.6
            'snr_high': 15,   # dB
            'snr_low': 5      # dB
        }
    
    def calculate_features(self, x_enc):
        """ç‰¹å¾æå–ä¸»å‡½æ•°"""
        coeffs = ptwt.wavedec(x_enc.reshape(-1, 1, x_enc.shape[-1]).float(), 
                             'db4', level=3, mode='reflect')
        
        # 1. é¢‘åŸŸç‰¹å¾ï¼ˆèƒ½é‡+ç†µï¼‰
        freq_features = self._extract_frequency_features(coeffs)
        
        # 2. è¶‹åŠ¿ç‰¹å¾ï¼ˆå¤šå°ºåº¦ä¸€è‡´æ€§ï¼‰
        trend_features = self._extract_trend_features(coeffs)
        
        # 3. ä¿¡å·è´¨é‡ï¼ˆSNRï¼‰
        quality_features = self._extract_quality_features(coeffs)
        
        # 4. é¢„æµ‹éš¾åº¦ï¼ˆç»¼åˆè¯„åˆ†ï¼‰
        difficulty = self._calculate_difficulty(
            freq_features, trend_features, quality_features
        )
        
        return {
            **freq_features,
            **trend_features,
            **quality_features,
            'difficulty': difficulty
        }
    
    def build_prompt(self, x_enc, min_val, max_val, median, lags):
        """æ„å»ºä¼˜åŒ–çš„prompt"""
        features = self.calculate_features(x_enc)
        
        if self.compression == 'minimal':
            return self._build_minimal_prompt(...)
        elif self.compression == 'balanced':
            return self._build_balanced_prompt(...)
        else:
            return self._build_detailed_prompt(...)
```

### 3.2 ä¸‰ç§å‹ç¼©çº§åˆ«å¯¹æ¯”

| çº§åˆ« | Tokenæ•° | é€‚ç”¨åœºæ™¯ | ä¿¡æ¯å®Œæ•´åº¦ |
|------|---------|----------|-----------|
| **Minimal** | ~70 (+1%) | ç®€å•å¹³ç¨³åºåˆ—ï¼ŒTokenå—é™ | â­â­â­ |
| **Balanced** | ~78 (+13%) | å¤§å¤šæ•°åœºæ™¯ï¼Œæ¨èé»˜è®¤ | â­â­â­â­ |
| **Detailed** | ~88 (+28%) | å¤æ‚åºåˆ—ï¼Œéœ€è¦è¯¦ç»†æŒ‡å¯¼ | â­â­â­â­â­ |

---

## å››ã€å®æ–½å»ºè®®ä¸æ³¨æ„äº‹é¡¹

### 4.1 åˆ†é˜¶æ®µå®æ–½ç­–ç•¥

**Week 1: åŸºç¡€ç‰ˆ**
```python
# å®ç°åŸºç¡€ç‰¹å¾æå–ï¼Œä½¿ç”¨å›ºå®šé˜ˆå€¼
features = calculate_wavelet_features_B(x_enc)
# æµ‹è¯•åŸºå‡†æ€§èƒ½
```

**Week 2: é˜ˆå€¼ä¼˜åŒ–**
```python
# åœ¨éªŒè¯é›†ä¸Šç»Ÿè®¡è‡ªé€‚åº”é˜ˆå€¼
thresholds = calculate_adaptive_thresholds(val_loader)
# å¯¹æ¯”å›ºå®švsè‡ªé€‚åº”
```

**Week 3: è¯­ä¹‰ä¼˜åŒ–**
```python
# å®éªŒåŠŸèƒ½æ€§æè¿°
# A/Bæµ‹è¯•ä¸åŒçš„è¯­ä¹‰æ˜ å°„
```

**Week 4: Tokenä¼˜åŒ–**
```python
# å®ç°ä¸‰ç§å‹ç¼©çº§åˆ«
# æµ‹è¯•å‹ç¼©å¯¹æ€§èƒ½çš„å½±å“
```

### 4.2 å…³é”®å†³ç­–ç‚¹

#### å†³ç­–1: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼ï¼Ÿ
- âœ… **æ¨è**: å¤šæ•°æ®é›†å®éªŒ â†’ ä½¿ç”¨è‡ªé€‚åº”
- âŒ **ä¸æ¨è**: å•æ•°æ®é›†å¿«é€ŸéªŒè¯ â†’ å›ºå®šé˜ˆå€¼

#### å†³ç­–2: æ˜¯å¦ä½¿ç”¨åŠŸèƒ½æ€§æè¿°ï¼Ÿ
- âœ… **æ¨è**: LLMæ”¯æŒinstruction-following â†’ å°è¯•åŠŸèƒ½æ€§
- âŒ **ä¸æ¨è**: ä½¿ç”¨BERTç­‰encoder-only â†’ æè¿°æ€§æ›´å¥½

#### å†³ç­–3: å‹ç¼©çº§åˆ«é€‰æ‹©ï¼Ÿ
- **Balanced** (é»˜è®¤): é€‚åˆå¤§å¤šæ•°åœºæ™¯
- **Minimal**: Tokené¢„ç®—<80æ—¶
- **Detailed**: å¤æ‚å¤šå˜é‡åºåˆ—

### 4.3 æ½œåœ¨é£é™©

| é£é™© | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|---------|
| é˜ˆå€¼æ³›åŒ–æ€§å·® | ä¸­ | ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼ + å¤šæ•°æ®é›†éªŒè¯ |
| LLMä¸ç†è§£æ–°è¯­ä¹‰ | ä½ | ä½¿ç”¨é¢„è®­ç»ƒè¯­æ–™ä¸­çš„å¸¸è§è¯æ±‡ |
| Tokenå¢åŠ å½±å“é€Ÿåº¦ | ä½ | +16 tokenså½±å“å¯å¿½ç•¥ (~2%) |
| è¿‡åº¦å·¥ç¨‹åŒ– | ä¸­ | å…ˆå®ç°åŸºç¡€ç‰ˆï¼ŒéªŒè¯æœ‰æ•ˆåå†ä¼˜åŒ– |

---

## äº”ã€æœ€ç»ˆæ¨èé…ç½®

```python
# æ¨èçš„æ–¹æ¡ˆBé…ç½®
config = {
    'dwt_level': 3,
    'wavelet': 'db4',
    'use_adaptive_thresholds': True,      # âœ… æ¨è
    'use_functional_desc': False,         # âš ï¸  å®éªŒæ€§
    'compression_level': 'balanced',      # âœ… é»˜è®¤
    'feature_groups': {
        'frequency': True,                # å¿…é¡»
        'trend': True,                    # å¿…é¡»
        'quality': True,                  # æ¨èï¼ˆSNRï¼‰
        'difficulty': True                # æ¨è
    },
    'semantic_mapping': 'descriptive',    # 'descriptive' | 'functional'
    'max_tokens': 85                      # Tokené¢„ç®—
}
```

**è¿™ä¸ªé…ç½®æä¾›äº†æ–¹æ¡ˆBçš„æœ€ä½³å¹³è¡¡ç‚¹ï¼šä¿¡æ¯ä¸°å¯Œ + å®ç°å¯è¡Œ + Tokenå¯æ§ï¼**

éœ€è¦æˆ‘æŒ‰ç…§è¿™ä¸ªä¼˜åŒ–ç‰ˆæœ¬å¼€å§‹ç¼–å†™å®Œæ•´ä»£ç å—ï¼Ÿ