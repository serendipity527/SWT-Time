# 训练时间预估工具使用指南

## 概述

`estimate_training_time.py` 是一个训练时间预估工具,通过运行少量训练批次来测量实际速度,然后预估整体训练所需时间。

## 功能特点

- **实际测量**: 通过运行真实的训练批次来测量速度,比理论计算更准确
- **详细分解**: 分别预估训练、验证、测试各阶段的时间
- **多维度统计**: 提供单epoch、总训练、多次迭代的时间预估
- **结果保存**: 自动保存详细的预估结果到JSON文件

## 使用方法

### 1. 基础使用

```bash
# 预估TimeLLM模型在ETTm1数据集上的训练时间
python estimate_training_time.py \
    --model TimeLLM \
    --data ETTm1 \
    --root_path ./dataset \
    --data_path ETTm1.csv \
    --train_epochs 10 \
    --batch_size 32
```

### 2. 使用SWT的配置

```bash
# 预估使用SWT优化的训练时间
python estimate_training_time.py \
    --model TimeLLM \
    --data ETTm1 \
    --root_path ./dataset \
    --data_path ETTm1.csv \
    --train_epochs 10 \
    --batch_size 32 \
    --use_swt \
    --swt_level 3 \
    --swt_wavelet db4
```

### 3. 使用DWT Prompt的配置

```bash
# 预估使用DWT动态prompt的训练时间
python estimate_training_time.py \
    --model TimeLLM \
    --data ETTm1 \
    --root_path ./dataset \
    --data_path ETTm1.csv \
    --train_epochs 10 \
    --batch_size 32 \
    --use_dwt_prompt \
    --dwt_prompt_level 3 \
    --prompt_compression balanced
```

### 4. 使用混合精度训练

```bash
# 预估使用AMP混合精度训练的时间
python estimate_training_time.py \
    --model TimeLLM \
    --data ETTm1 \
    --root_path ./dataset \
    --data_path ETTm1.csv \
    --train_epochs 10 \
    --batch_size 32 \
    --use_amp
```

### 5. 使用DeepSpeed

```bash
# 预估使用DeepSpeed的训练时间
python estimate_training_time.py \
    --model TimeLLM \
    --data ETTm1 \
    --root_path ./dataset \
    --data_path ETTm1.csv \
    --train_epochs 10 \
    --batch_size 32 \
    --use_deepspeed
```

### 6. 调整测量精度

```bash
# 增加测量批次数以提高预估精度
python estimate_training_time.py \
    --model TimeLLM \
    --data ETTm1 \
    --root_path ./dataset \
    --data_path ETTm1.csv \
    --train_epochs 10 \
    --batch_size 32 \
    --warmup_batches 10 \
    --measure_batches 20
```

## 完整配置示例

```bash
# ETTm1数据集,96->96预测,使用SWT和DWT Prompt
python estimate_training_time.py \
    --model TimeLLM \
    --data ETTm1 \
    --root_path ./dataset \
    --data_path ETT-small/ETTh1.csv \
    --features M \
    --seq_len 96 \
    --label_len 48 \
    --pred_len 96 \
    --enc_in 7 \
    --dec_in 7 \
    --c_out 7 \
    --d_model 16 \
    --n_heads 8 \
    --e_layers 2 \
    --d_layers 1 \
    --d_ff 32 \
    --train_epochs 10 \
    --itr 3 \
    --batch_size 32 \
    --learning_rate 0.0001 \
    --llm_model LLAMA \
    --llm_dim 4096 \
    --llm_layers 6 \
    --use_swt \
    --swt_level 3 \
    --swt_wavelet db4 \
    --use_dwt_prompt \
    --dwt_prompt_level 3 \
    --prompt_compression balanced \
    --warmup_batches 5 \
    --measure_batches 10
```

## 输出说明

### 终端输出

脚本会在终端输出详细的预估结果,包括:

1. **模型信息**: 模型名称、参数量
2. **数据信息**: 各数据集的批次数
3. **训练配置**: epoch数、batch size、是否使用SWT等
4. **测量结果**: 平均每批次训练/验证时间
5. **单个Epoch时间预估**: 训练、验证、测试各阶段时间
6. **总体时间预估**: 完整训练所需总时间
7. **额外信息**: 每小时/每天可完成的epoch数

### JSON文件输出

详细结果会保存到 `training_time_estimate.json`,包含:

```json
{
    "model_info": {
        "model_name": "TimeLLM",
        "total_params": 1234567,
        "trainable_params": 1234567
    },
    "data_info": {
        "train_batches": 100,
        "vali_batches": 20,
        "test_batches": 20
    },
    "training_config": {
        "train_epochs": 10,
        "iterations": 1,
        "use_amp": false,
        "use_swt": true,
        "use_dwt_prompt": false
    },
    "measured_times": {
        "avg_train_batch_time": 0.5,
        "avg_vali_batch_time": 0.3
    },
    "time_estimates": {
        "train_time_per_epoch_seconds": 50.0,
        "vali_time_per_epoch_seconds": 6.0,
        "test_time_per_epoch_seconds": 6.0,
        "total_time_per_epoch_seconds": 62.0,
        "total_training_time_seconds": 620.0,
        "total_time_all_itr_seconds": 620.0
    }
}
```

## 参数说明

### 测量参数

- `--warmup_batches`: 预热批次数,默认5。这些批次不计入时间统计,用于让模型进入稳定状态
- `--measure_batches`: 测量批次数,默认10。用于计算平均时间的批次数,越多越准确但耗时越长
- `--use_deepspeed`: 是否使用DeepSpeed进行预估

### 主要训练参数

所有 `run_main.py` 支持的参数都可以使用,主要包括:

- 模型配置: `--model`, `--d_model`, `--n_heads`, `--e_layers`等
- 数据配置: `--data`, `--root_path`, `--data_path`, `--seq_len`, `--pred_len`等
- 训练配置: `--train_epochs`, `--batch_size`, `--learning_rate`, `--itr`等
- SWT配置: `--use_swt`, `--swt_level`, `--swt_wavelet`等
- DWT Prompt配置: `--use_dwt_prompt`, `--dwt_prompt_level`, `--prompt_compression`等

## 注意事项

1. **GPU内存**: 确保GPU内存足够运行指定的batch_size
2. **数据路径**: 确保数据集路径正确,数据文件存在
3. **测量精度**: `measure_batches`越多,预估越准确,但运行时间越长
4. **预估误差**: 实际训练时间可能会有10-20%的误差,因为:
   - 随着训练进行,梯度计算可能变化
   - 数据加载速度可能不稳定
   - GPU调度可能有波动
5. **Early Stopping**: 如果使用early stopping,实际训练可能提前结束

## 建议

1. **首次使用**: 建议先用较小的`measure_batches`(如5-10)快速预估
2. **精确预估**: 如果需要更准确的预估,可以增加到20-30个批次
3. **对比实验**: 在进行超参数调优前,可以先预估不同配置的训练时间
4. **资源规划**: 根据预估结果合理安排GPU使用时间和实验计划

## 常见问题

### Q: 预估时间比实际训练时间短?
A: 可能原因:
- 数据加载成为瓶颈(增加`--num_workers`)
- GPU使用率不稳定
- 包含了checkpoint保存等额外开销

### Q: 预估时间比实际训练时间长?
A: 可能原因:
- 测量阶段GPU还在预热
- 增加`--warmup_batches`数量
- 某些优化(如JIT编译)在后续训练中生效

### Q: 如何提高预估精度?
A: 
- 增加`--measure_batches`数量
- 增加`--warmup_batches`数量
- 在实际训练环境(相同GPU、相同负载)下运行预估

## 示例场景

### 场景1: 快速预估
适用于快速了解大概训练时间:
```bash
python estimate_training_time.py \
    --model TimeLLM \
    --data ETTm1 \
    --train_epochs 10 \
    --warmup_batches 3 \
    --measure_batches 5
```

### 场景2: 精确预估
适用于制定详细的训练计划:
```bash
python estimate_training_time.py \
    --model TimeLLM \
    --data ETTm1 \
    --train_epochs 10 \
    --warmup_batches 10 \
    --measure_batches 30
```

### 场景3: 对比不同配置
预估不同batch size的影响:
```bash
# 小batch size
python estimate_training_time.py --batch_size 16 ...

# 大batch size  
python estimate_training_time.py --batch_size 64 ...
```

## 技术细节

脚本的工作流程:

1. **初始化**: 加载数据、创建模型、配置优化器
2. **预热**: 运行`warmup_batches`个批次,让GPU进入稳定状态
3. **测量训练**: 运行`measure_batches`个批次,记录每个批次的时间
4. **测量验证**: 运行部分验证批次,测量验证速度
5. **计算预估**: 根据测量结果和训练配置计算总时间
6. **输出结果**: 在终端显示并保存到JSON文件

## 相关文件

- `estimate_training_time.py`: 主脚本
- `training_time_estimate.json`: 输出的详细预估结果
- `run_main.py`: 实际训练脚本,参数配置基本一致
