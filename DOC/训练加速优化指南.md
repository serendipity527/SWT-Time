# è®­ç»ƒåŠ é€Ÿä¼˜åŒ–æŒ‡å—

## å½“å‰æ€§èƒ½åˆ†æ

æ ¹æ®é¢„ä¼°ç»“æœ:
- **å¹³å‡è®­ç»ƒæ‰¹æ¬¡æ—¶é—´**: 0.2744ç§’/batch
- **å•ä¸ªEpochæ—¶é—´**: 56åˆ†é’Ÿ
- **10ä¸ªEpochæ€»æ—¶é—´**: 9å°æ—¶20åˆ†é’Ÿ
- **3æ¬¡è¿­ä»£æ€»æ—¶é—´**: 28å°æ—¶

**ä¸»è¦ç“¶é¢ˆ**:
- æœªå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ(AMP)
- batch_sizeè¾ƒå°(32)
- æ¨¡å‹å‚æ•°é‡å¤§(1.33äº¿,å¯è®­ç»ƒ5089ä¸‡)

---

## ä¼˜åŒ–æ–¹æ¡ˆ(æŒ‰ä¼˜å…ˆçº§æ’åº)

### ğŸ”¥ æ–¹æ¡ˆ1: å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ(AMP) - **å¼ºçƒˆæ¨è**

**é¢„æœŸåŠ é€Ÿ**: 1.5-3å€  
**å®æ–½éš¾åº¦**: â­(æç®€å•)  
**æ•ˆæœ**: â­â­â­â­â­

#### æ“ä½œæ–¹æ³•:

è®­ç»ƒæ—¶æ·»åŠ  `--use_amp` å‚æ•°:

```bash
python run_main.py \
    --use_amp \
    # ... å…¶ä»–å‚æ•°
```

#### é¢„æœŸæ•ˆæœ:

- **ä¼˜åŒ–å‰**: 56åˆ†é’Ÿ/epoch
- **ä¼˜åŒ–å**: 19-37åˆ†é’Ÿ/epoch
- **10ä¸ªepoch**: 3-6å°æ—¶(vs åŸ9å°æ—¶)

---

### ğŸš€ æ–¹æ¡ˆ2: å¢å¤§Batch Size - **æ¨è**

**é¢„æœŸåŠ é€Ÿ**: 1.2-1.5å€  
**å®æ–½éš¾åº¦**: â­â­(éœ€è¦GPUå†…å­˜)  
**æ•ˆæœ**: â­â­â­â­

#### å»ºè®®é…ç½®:

```bash
# å½“å‰: batch_size=32
# ä¼˜åŒ–: batch_size=64 (å¦‚æœGPUå†…å­˜è¶³å¤Ÿ)
# æ¿€è¿›: batch_size=128 (éœ€è¦å¤§æ˜¾å­˜æˆ–æ¢¯åº¦ç´¯ç§¯)

python run_main.py \
    --batch_size 64 \
    --use_amp \  # å»ºè®®åŒæ—¶å¯ç”¨AMPä»¥èŠ‚çœæ˜¾å­˜
    # ... å…¶ä»–å‚æ•°
```

#### å†…å­˜ä¸å¤Ÿæ€ä¹ˆåŠ?

ä½¿ç”¨**æ¢¯åº¦ç´¯ç§¯**æ¨¡æ‹Ÿå¤§batch:

```python
# åœ¨run_main.pyä¸­æ·»åŠ 
accumulation_steps = 4  # ç­‰æ•ˆäºbatch_size * 4

for i, batch in enumerate(train_loader):
    loss = model(batch)
    loss = loss / accumulation_steps  # å½’ä¸€åŒ–
    accelerator.backward(loss)
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

### âš¡ æ–¹æ¡ˆ3: ä¼˜åŒ–DeepSpeedé…ç½®

**é¢„æœŸåŠ é€Ÿ**: 1.3-2å€  
**å®æ–½éš¾åº¦**: â­â­â­  
**æ•ˆæœ**: â­â­â­â­

#### å½“å‰é…ç½®æ£€æŸ¥:

æŸ¥çœ‹ `ds_config_zero2.json`:

```json
{
  "train_micro_batch_size_per_gpu": "auto",
  "gradient_accumulation_steps": "auto",
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    }
  },
  "fp16": {
    "enabled": true  # å¯ç”¨æ··åˆç²¾åº¦
  },
  "bf16": {
    "enabled": false
  }
}
```

#### ä¼˜åŒ–å»ºè®®:

1. **å¦‚æœæœ‰A100ç­‰æ”¯æŒBF16çš„GPU**:
```json
{
  "fp16": {"enabled": false},
  "bf16": {"enabled": true}  # BF16æ›´ç¨³å®š
}
```

2. **å…³é—­CPU offload**(å¦‚æœGPUå†…å­˜å……è¶³):
```json
{
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "none"  # æ”¹ä¸ºnone
    }
  }
}
```

3. **å¯ç”¨æ¢¯åº¦checkpointing**(èŠ‚çœæ˜¾å­˜):
```json
{
  "activation_checkpointing": {
    "partition_activations": true,
    "cpu_checkpointing": false
  }
}
```

---

### ğŸ’¾ æ–¹æ¡ˆ4: ä¼˜åŒ–æ•°æ®åŠ è½½

**é¢„æœŸåŠ é€Ÿ**: 1.1-1.2å€  
**å®æ–½éš¾åº¦**: â­  
**æ•ˆæœ**: â­â­â­

#### è°ƒæ•´num_workers:

```bash
# å½“å‰: num_workers=10
# å»ºè®®æµ‹è¯•: 4, 8, 16

python run_main.py \
    --num_workers 8 \
    # ... å…¶ä»–å‚æ•°
```

#### å¯ç”¨pin_memory:

åœ¨ `data_factory.py` ä¸­ä¿®æ”¹:

```python
data_loader = DataLoader(
    data_set,
    batch_size=batch_size,
    shuffle=shuffle_flag,
    num_workers=args.num_workers,
    drop_last=drop_last,
    pin_memory=True,  # æ·»åŠ è¿™è¡Œ
    prefetch_factor=2  # æ·»åŠ è¿™è¡Œ
)
```

---

### ğŸ¯ æ–¹æ¡ˆ5: ä½¿ç”¨Torch Compile (PyTorch 2.0+)

**é¢„æœŸåŠ é€Ÿ**: 1.2-1.8å€  
**å®æ–½éš¾åº¦**: â­â­  
**æ•ˆæœ**: â­â­â­â­

#### å®æ–½æ–¹æ³•:

åœ¨æ¨¡å‹åˆå§‹åŒ–åæ·»åŠ :

```python
# åœ¨run_main.pyä¸­
model = TimeLLM.Model(args).float()

# æ·»åŠ ç¼–è¯‘
if torch.__version__ >= '2.0.0':
    model = torch.compile(model, mode='reduce-overhead')
    # modeé€‰é¡¹: 'default', 'reduce-overhead', 'max-autotune'
```

---

### ğŸ”§ æ–¹æ¡ˆ6: å‡å°‘éªŒè¯é¢‘ç‡

**é¢„æœŸåŠ é€Ÿ**: æ•´ä½“åŠ é€Ÿ  
**å®æ–½éš¾åº¦**: â­  
**æ•ˆæœ**: â­â­

å½“å‰æ¯ä¸ªepochéƒ½éªŒè¯ï¼Œå¯ä»¥æ”¹ä¸ºæ¯Nä¸ªepochéªŒè¯ä¸€æ¬¡:

```python
# åœ¨run_main.pyçš„è®­ç»ƒå¾ªç¯ä¸­
for epoch in range(args.train_epochs):
    # ... è®­ç»ƒä»£ç 
    
    # åªåœ¨ç‰¹å®šepochéªŒè¯
    if (epoch + 1) % 2 == 0 or (epoch + 1) == args.train_epochs:
        vali_loss = vali(...)
        test_loss = vali(...)
```

---

### ğŸ¨ æ–¹æ¡ˆ7: æ¨¡å‹æ¶æ„ä¼˜åŒ–

**é¢„æœŸåŠ é€Ÿ**: 1.2-2å€  
**å®æ–½éš¾åº¦**: â­â­â­â­  
**æ•ˆæœ**: â­â­â­

#### 7.1 å‡å°‘LLMå±‚æ•°

```bash
# å½“å‰: llm_layers=6
# ä¼˜åŒ–: llm_layers=4 æˆ– 3

python run_main.py \
    --llm_layers 4 \
    # ... å…¶ä»–å‚æ•°
```

#### 7.2 ä½¿ç”¨Flash Attention

åœ¨ `TimeLLM.py` ä¸­å¯ç”¨:

```python
from torch.nn.functional import scaled_dot_product_attention

# åœ¨attentionè®¡ç®—æ—¶ä½¿ç”¨
attn_output = scaled_dot_product_attention(
    query, key, value,
    is_causal=False
)
```

#### 7.3 å†»ç»“LLMéƒ¨åˆ†å‚æ•°

```python
# åœ¨æ¨¡å‹åˆå§‹åŒ–å
for name, param in model.named_parameters():
    if 'llm' in name.lower():
        param.requires_grad = False  # å†»ç»“LLM
```

---

## ğŸ¯ æ¨èç»„åˆæ–¹æ¡ˆ

### æ–¹æ¡ˆA: å¿«é€Ÿä¼˜åŒ–(æœ€å°æ”¹åŠ¨,æœ€å¤§æ”¶ç›Š)

```bash
python run_main.py \
    --task_name long_term_forecast \
    --is_training 1 \
    --model_id test \
    --model_comment fast_training \
    --model TimeLLM \
    --data ETTm1 \
    --root_path ./dataset \
    --data_path ETT-small/ETTm1.csv \
    --features M \
    --seq_len 96 \
    --pred_len 96 \
    --llm_model GPT2 \
    --llm_dim 768 \
    --llm_layers 6 \
    --use_amp \                    # â† å¯ç”¨AMP
    --batch_size 64 \              # â† å¢å¤§batch size
    --num_workers 8 \              # â† ä¼˜åŒ–æ•°æ®åŠ è½½
    --use_swt \
    --use_dwt_prompt
```

**é¢„æœŸæ•ˆæœ**: 
- è®­ç»ƒæ—¶é—´: **15-25åˆ†é’Ÿ/epoch** (vs åŸ56åˆ†é’Ÿ)
- åŠ é€Ÿæ¯”: **2-3.7å€**

---

### æ–¹æ¡ˆB: æ¿€è¿›ä¼˜åŒ–(éœ€è¦ä»£ç ä¿®æ”¹)

åŸºäºæ–¹æ¡ˆA,é¢å¤–æ·»åŠ :

1. ä¿®æ”¹ `ds_config_zero2.json`:
```json
{
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {"device": "none"}
  },
  "bf16": {"enabled": true}  # å¦‚æœGPUæ”¯æŒ
}
```

2. åœ¨ `run_main.py` ä¸­æ·»åŠ :
```python
# ä½¿ç”¨torch.compile
if torch.__version__ >= '2.0.0':
    model = torch.compile(model, mode='reduce-overhead')

# æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 2
```

3. å‡å°‘éªŒè¯é¢‘ç‡(æ¯2ä¸ªepochéªŒè¯ä¸€æ¬¡)

**é¢„æœŸæ•ˆæœ**:
- è®­ç»ƒæ—¶é—´: **10-15åˆ†é’Ÿ/epoch** (vs åŸ56åˆ†é’Ÿ)
- åŠ é€Ÿæ¯”: **3.7-5.6å€**

---

## å®æ–½æ­¥éª¤

### Step 1: æµ‹è¯•AMPæ•ˆæœ

```bash
# å…ˆç”¨å°æ•°æ®é›†æµ‹è¯•
python estimate_training_time.py \
    --model TimeLLM \
    --data ETTm1 \
    --use_amp \
    --batch_size 32 \
    --measure_batches 10
```

### Step 2: æµ‹è¯•å¤§batch size

```bash
# é€æ­¥å¢å¤§batch size,ç›‘æ§GPUå†…å­˜
python estimate_training_time.py \
    --use_amp \
    --batch_size 64  # ç„¶åå°è¯•128
```

### Step 3: å®Œæ•´è®­ç»ƒæµ‹è¯•

```bash
# ä½¿ç”¨ä¼˜åŒ–é…ç½®è¿è¡Œ1-2ä¸ªepochéªŒè¯
python run_main.py \
    --train_epochs 2 \
    --use_amp \
    --batch_size 64 \
    # ... å…¶ä»–å‚æ•°
```

---

## ç›‘æ§ä¸è°ƒè¯•

### ç›‘æ§GPUä½¿ç”¨ç‡

```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æˆ–ä½¿ç”¨
nvitop
```

### ç›‘æ§è®­ç»ƒé€Ÿåº¦

åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ profiler:

```python
from torch.profiler import profile, ProfilerActivity

with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
    # è®­ç»ƒä¸€ä¸ªbatch
    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
    
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

---

## å¸¸è§é—®é¢˜

### Q: å¯ç”¨AMPålosså˜æˆNaN?
A: 
- é™ä½å­¦ä¹ ç‡: `--learning_rate 0.00005`
- ä½¿ç”¨BF16ä»£æ›¿FP16
- æ·»åŠ æ¢¯åº¦è£å‰ª: `torch.nn.utils.clip_grad_norm_()`

### Q: batch_sizeå¢å¤§åOOM?
A:
- å¯ç”¨ `--use_amp`
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
- å‡å°‘ `llm_layers`
- å¯ç”¨gradient checkpointing

### Q: æ•°æ®åŠ è½½æ…¢æ€ä¹ˆåŠ?
A:
- è°ƒæ•´ `num_workers` (å»ºè®®4-16)
- å¯ç”¨ `pin_memory=True`
- ä½¿ç”¨SSDå­˜å‚¨æ•°æ®
- é¢„å…ˆç¼“å­˜å¤„ç†å¥½çš„æ•°æ®

---

## æ€§èƒ½å¯¹æ¯”é¢„ä¼°

| é…ç½® | æ—¶é—´/Epoch | 10 Epochs | åŠ é€Ÿæ¯” |
|------|-----------|-----------|--------|
| åŸå§‹é…ç½® | 56åˆ†é’Ÿ | 9.3å°æ—¶ | 1.0x |
| +AMP | 19-28åˆ†é’Ÿ | 3.2-4.7å°æ—¶ | 2-3x |
| +AMP+BS64 | 15-25åˆ†é’Ÿ | 2.5-4.2å°æ—¶ | 2.2-3.7x |
| +å®Œæ•´ä¼˜åŒ– | 10-15åˆ†é’Ÿ | 1.7-2.5å°æ—¶ | 3.7-5.6x |

---

## æ€»ç»“

**ç«‹å³å¯ç”¨çš„ä¼˜åŒ–**(æ— éœ€æ”¹ä»£ç ):
```bash
--use_amp --batch_size 64 --num_workers 8
```

**é¢„æœŸæ•ˆæœ**: è®­ç»ƒæ—¶é—´å‡å°‘ **60-70%**

**ä¸‹ä¸€æ­¥**: æ ¹æ®å®é™…æµ‹è¯•ç»“æœ,é€‰æ‹©æ€§åº”ç”¨å…¶ä»–ä¼˜åŒ–æ–¹æ¡ˆ
