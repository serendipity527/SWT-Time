import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional, Literal
import warnings

try:
    import ptwt
    PTWT_AVAILABLE = True
except ImportError:
    PTWT_AVAILABLE = False
    warnings.warn(
        "ptwtåº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install ptwt\n"
        "è¿™å°†å¯ç”¨GPUåŠ é€Ÿçš„å°æ³¢å˜æ¢åŠŸèƒ½ã€‚"
    )


class ReplicationPad1d(nn.Module):
    """å¤åˆ¶å¡«å……å±‚ï¼Œç”¨äºæ—¶åºæ•°æ®çš„è¾¹ç•Œå¡«å……
    
    åœ¨æ—¶é—´åºåˆ—æœ«å°¾è¿›è¡Œå¤åˆ¶å¡«å……ï¼Œé¿å…å¼•å…¥é›¶å€¼çªå˜ï¼Œä¿æŒä¿¡å·è¿ç»­æ€§ã€‚
    
    Args:
        padding: (left_pad, right_pad) å…ƒç»„ï¼ŒæŒ‡å®šå·¦å³å¡«å……çš„é•¿åº¦
    
    Input:
        x: (B, N, T) - [batch_size, num_variables, time_steps]
    
    Output:
        (B, N, T + right_pad) - æœ«å°¾å¤åˆ¶å¡«å……åçš„åºåˆ—
    
    ç¤ºä¾‹:
        >>> pad = ReplicationPad1d((0, 8))
        >>> x = torch.randn(4, 7, 512)
        >>> out = pad(x)
        >>> print(out.shape)  # torch.Size([4, 7, 520])
    """
    def __init__(self, padding: Tuple[int, int]) -> None:
        super(ReplicationPad1d, self).__init__()
        self.padding = padding  # (left_pad, right_pad)
    
    def forward(self, input: torch.Tensor) -> torch.Tensor:
        """
        Args:
            input: (B, N, T) - è¾“å…¥æ—¶é—´åºåˆ—
        
        Returns:
            (B, N, T + padding[1]) - å¡«å……åçš„åºåˆ—
        
        ç»´åº¦å˜æ¢:
            (B, N, T) 
            -> å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥ (B, N, 1)
            -> å¤åˆ¶ padding[1] æ¬¡ (B, N, padding[1])
            -> æ‹¼æ¥ (B, N, T + padding[1])
        """
        if self.padding[1] > 0:
            # æå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥å¹¶å¤åˆ¶
            last_value = input[:, :, -1].unsqueeze(-1)  # (B, N, 1)
            replicate_padding = last_value.repeat(1, 1, self.padding[1])  # (B, N, padding[1])
            output = torch.cat([input, replicate_padding], dim=-1)  # (B, N, T+padding[1])
        else:
            output = input
        
        return output


class SWTDecomposition(nn.Module):
    """å¹³ç¨³å°æ³¢å˜æ¢(SWT)åˆ†è§£æ¨¡å— - GPUåŠ é€Ÿç‰ˆæœ¬
    
    ä½¿ç”¨ptwtåº“å®ç°ï¼Œæ”¯æŒGPUåŠ é€Ÿå’Œæ‰¹é‡å¤„ç†ã€‚
    
    ç‰¹ç‚¹ï¼š
    1. æ— é™é‡‡æ ·ï¼Œä¿æŒåºåˆ—é•¿åº¦ä¸å˜ï¼ˆå¹³ç¨³å°æ³¢å˜æ¢ï¼‰
    2. å¹³ç§»ä¸å˜æ€§ï¼ˆTranslation Invarianceï¼‰
    3. GPUåŸç”Ÿæ”¯æŒï¼Œé«˜æ•ˆæ‰¹é‡å¤„ç†
    4. æå–å¤šå°ºåº¦é¢‘åŸŸç‰¹å¾
    
    Args:
        wavelet: å°æ³¢åŸºå‡½æ•°åç§° (å¦‚ 'db4', 'haar', 'sym4', 'coif1' ç­‰)
        level: SWTåˆ†è§£å±‚æ•°ï¼Œæ¨è2-4å±‚
               level=1: 2ä¸ªé¢‘æ®µ (1ä¸ªè¿‘ä¼¼ + 1ä¸ªç»†èŠ‚)
               level=2: 3ä¸ªé¢‘æ®µ (1ä¸ªè¿‘ä¼¼ + 2ä¸ªç»†èŠ‚)
               level=3: 4ä¸ªé¢‘æ®µ (1ä¸ªè¿‘ä¼¼ + 3ä¸ªç»†èŠ‚)
    
    æ³¨æ„ï¼šptwtåº“é»˜è®¤ä½¿ç”¨zero-paddingè¾¹ç•Œæ¨¡å¼
    
    Input:
        x: (B, N, T) - [batch_size, num_variables, time_steps]
    
    Output:
        coeffs: (B, N, T, Level+1) - å¤šé¢‘æ®µç³»æ•°å †å 
                æœ€åä¸€ç»´çš„æ’åˆ—é¡ºåºï¼š[cA_n, cD_n, cD_{n-1}, ..., cD_1]
                - cA_n: ç¬¬nå±‚è¿‘ä¼¼ç³»æ•°ï¼ˆæœ€ä½é¢‘ï¼Œå…¨å±€è¶‹åŠ¿ï¼‰
                - cD_n: ç¬¬nå±‚ç»†èŠ‚ç³»æ•°ï¼ˆæœ€é«˜é¢‘æ®µï¼‰
                - cD_1: ç¬¬1å±‚ç»†èŠ‚ç³»æ•°ï¼ˆæœ€ä½é¢‘æ®µçš„ç»†èŠ‚ï¼‰
    
    ç¤ºä¾‹ï¼š
        >>> swt = SWTDecomposition(wavelet='db4', level=3)
        >>> x = torch.randn(8, 7, 512)  # batch=8, vars=7, time=512
        >>> coeffs = swt(x)
        >>> print(coeffs.shape)  # torch.Size([8, 7, 512, 4])
    """
    
    def __init__(self, 
                 wavelet: str = 'db4', 
                 level: int = 3):
        super(SWTDecomposition, self).__init__()
        
        # æ£€æŸ¥ptwtåº“æ˜¯å¦å¯ç”¨
        if not PTWT_AVAILABLE:
            raise ImportError(
                "ptwtåº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨GPUåŠ é€Ÿçš„SWTã€‚\n"
                "è¯·è¿è¡Œ: pip install ptwt"
            )
        
        self.wavelet_name = wavelet
        self.level = level
        
        # éªŒè¯å°æ³¢åç§°æ˜¯å¦æœ‰æ•ˆï¼ˆptwtæ”¯æŒçš„å°æ³¢ç±»å‹ï¼‰
        # ptwtç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å½¢å¼çš„å°æ³¢åç§°ï¼Œä¸éœ€è¦å®ä¾‹åŒ–Waveletå¯¹è±¡
        valid_wavelets = [
            'haar', 'db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8',
            'sym2', 'sym3', 'sym4', 'sym5', 'sym6', 'sym7', 'sym8',
            'coif1', 'coif2', 'coif3', 'coif4', 'coif5'
        ]
        if wavelet not in valid_wavelets:
            warnings.warn(
                f"å°æ³¢ '{wavelet}' å¯èƒ½ä¸è¢«ptwtæ”¯æŒã€‚\n"
                f"å¸¸ç”¨çš„å°æ³¢ç±»å‹: {', '.join(valid_wavelets[:10])}..."
            )
        
        # æœ€å°åºåˆ—é•¿åº¦ï¼ˆSWTè¦æ±‚è‡³å°‘ä¸º2^levelï¼‰
        self.min_length = 2 ** self.level
    
    def _validate_input(self, x: torch.Tensor) -> None:
        """éªŒè¯è¾“å…¥å¼ é‡çš„åˆæ³•æ€§
        
        Args:
            x: è¾“å…¥å¼ é‡ (B, N, T)
        
        Raises:
            ValueError: å¦‚æœè¾“å…¥ä¸æ»¡è¶³è¦æ±‚
        """
        if x.ndim != 3:
            raise ValueError(
                f"è¾“å…¥å¿…é¡»æ˜¯3ç»´å¼ é‡ (Batch, N_vars, Time)ï¼Œå½“å‰ç»´åº¦: {x.ndim}"
            )
        
        B, N, T = x.shape
        
        # æ£€æŸ¥åºåˆ—é•¿åº¦
        if T < self.min_length:
            raise ValueError(
                f"åºåˆ—é•¿åº¦ {T} å¤ªçŸ­ï¼ŒSWT({self.level}å±‚)è‡³å°‘éœ€è¦ {self.min_length} ä¸ªæ—¶é—´æ­¥"
            )
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«NaNæˆ–Inf
        if torch.isnan(x).any() or torch.isinf(x).any():
            raise ValueError("è¾“å…¥åŒ…å«NaNæˆ–Infå€¼ï¼Œè¯·å…ˆè¿›è¡Œæ•°æ®æ¸…æ´—")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        æ‰§è¡Œå¹³ç¨³å°æ³¢å˜æ¢
        
        Args:
            x: (B, N, T) - è¾“å…¥æ—¶é—´åºåˆ—
               B: batchå¤§å°
               N: å˜é‡æ•°é‡ï¼ˆå¤šå˜é‡æ—¶åºï¼‰
               T: æ—¶é—´æ­¥é•¿åº¦
        
        Returns:
            coeffs: (B, N, T, Level+1) - å¤šå°ºåº¦ç³»æ•°
                    æœ€åä¸€ç»´åŒ…å«æ‰€æœ‰é¢‘æ®µï¼š
                    - coeffs[..., 0]: è¿‘ä¼¼ç³»æ•° cA_n (ä½é¢‘è¶‹åŠ¿)
                    - coeffs[..., 1]: ç»†èŠ‚ç³»æ•° cD_n (æœ€é«˜é¢‘)
                    - coeffs[..., 2]: ç»†èŠ‚ç³»æ•° cD_{n-1}
                    - ...
                    - coeffs[..., n]: ç»†èŠ‚ç³»æ•° cD_1 (æœ€ä½é¢‘ç»†èŠ‚)
        
        ç»´åº¦å˜æ¢æµç¨‹ï¼š
            (B, N, T) 
            -> é€å±‚SWTåˆ†è§£
            -> æ”¶é›† [cA_level, cD_level, cD_{level-1}, ..., cD_1]
            -> Stack åˆ°æ–°ç»´åº¦
            -> (B, N, T, Level+1)
        """
        # 1. è¾“å…¥éªŒè¯
        self._validate_input(x)
        
        B, N, T = x.shape
        dtype = x.dtype
        
        # 1.5 æ•°æ®ç±»å‹è½¬æ¢ï¼šptwtä¸æ”¯æŒbfloat16ï¼Œéœ€è¦è½¬æ¢ä¸ºfloat32
        if dtype == torch.bfloat16:
            x = x.float()
            convert_back_to_bfloat16 = True
        else:
            convert_back_to_bfloat16 = False
        
        # 2. Reshape: (B, N, T) -> (B*N, T) 
        # ptwtçš„swtå‡½æ•°è¦æ±‚è¾“å…¥ä¸º2D: (batch, time)
        x_reshaped = x.reshape(B * N, T)  # (B*N, T)
        
        # 3. æ‰§è¡ŒSWTåˆ†è§£ï¼ˆGPUåŠ é€Ÿï¼‰
        try:
            # ptwt.swtè¿”å›åˆ—è¡¨: [(cA_n, cD_n), (cA_{n-1}, cD_{n-1}), ..., (cA_1, cD_1)]
            # æ³¨æ„ï¼šè¿™é‡Œçš„é¡ºåºæ˜¯ä»æœ€é«˜å±‚åˆ°æœ€ä½å±‚
            # ptwté»˜è®¤ä½¿ç”¨zero-paddingè¾¹ç•Œæ¨¡å¼
            coeffs_list = ptwt.swt(
                x_reshaped,           # (B*N, T)
                self.wavelet_name,    # å°æ³¢åç§°å­—ç¬¦ä¸²
                level=self.level      # åˆ†è§£å±‚æ•°
            )
            
            # coeffs_listæ ¼å¼è¯´æ˜ï¼š
            # level=3æ—¶: [(cA3, cD3), (cA2, cD2), (cA1, cD1)]
            # æˆ‘ä»¬åªéœ€è¦æœ€é¡¶å±‚çš„cAå’Œæ‰€æœ‰å±‚çš„cD
            
        except Exception as e:
            raise RuntimeError(
                f"SWTåˆ†è§£å¤±è´¥: {e}\n"
                f"è¾“å…¥å½¢çŠ¶: {x_reshaped.shape}, å°æ³¢: {self.wavelet_name}, "
                f"å±‚æ•°: {self.level}"
            )
        
        # 4. æå–å¹¶é‡ç»„ç³»æ•°
        # ç›®æ ‡ï¼š[cA_n, cD_n, cD_{n-1}, ..., cD_1]
        # 
        # ptwt.swtè¿”å›æ ¼å¼è¯´æ˜ï¼š
        # è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªtensor
        # å¯¹äºlevel=3: [cD1, cD2, cD3, cA3] (ä»ä½é¢‘åˆ°é«˜é¢‘çš„ç»†èŠ‚ï¼Œæœ€åæ˜¯è¿‘ä¼¼)
        all_bands = []
        
        # æ£€æŸ¥è¿”å›æ ¼å¼
        if not isinstance(coeffs_list, list):
            raise TypeError(f"ptwt.swtè¿”å›ç±»å‹é”™è¯¯: {type(coeffs_list)}")
        
        # ptwtè¿”å›: [cD1, cD2, ..., cDn, cAn]
        # æˆ‘ä»¬éœ€è¦: [cAn, cDn, cD(n-1), ..., cD1]
        
        # 4.1 æœ€åä¸€ä¸ªæ˜¯è¿‘ä¼¼ç³»æ•°cA
        cA_top = coeffs_list[-1]  # (B*N, T)
        all_bands.append(cA_top)
        
        # 4.2 å‰é¢çš„éƒ½æ˜¯ç»†èŠ‚ç³»æ•°ï¼Œä»åå¾€å‰å–ï¼ˆä»é«˜é¢‘åˆ°ä½é¢‘ï¼‰
        for i in range(len(coeffs_list) - 2, -1, -1):
            all_bands.append(coeffs_list[i])  # (B*N, T)
        
        # 5. å †å åˆ°æ–°ç»´åº¦
        # all_bands: list of (B*N, T), length = level+1
        # Stack: (B*N, T, Level+1)
        coeffs_stacked = torch.stack(all_bands, dim=-1)  # (B*N, T, Level+1)
        
        # 6. Reshapeå›åŸå§‹batchç»“æ„
        # (B*N, T, Level+1) -> (B, N, T, Level+1)
        coeffs_output = coeffs_stacked.reshape(B, N, T, self.level + 1)
        
        # 7. æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼ˆå¯é€‰ï¼Œè°ƒè¯•æ—¶æœ‰ç”¨ï¼‰
        if torch.isnan(coeffs_output).any():
            warnings.warn(
                "SWTåˆ†è§£ç»“æœåŒ…å«NaNå€¼ï¼Œå¯èƒ½æ˜¯æ•°å€¼ä¸ç¨³å®šæˆ–è¾“å…¥å¼‚å¸¸"
            )
        
        # 8. è½¬å›åŸå§‹æ•°æ®ç±»å‹
        if convert_back_to_bfloat16:
            coeffs_output = coeffs_output.bfloat16()
        
        return coeffs_output


class ISWTReconstruction(nn.Module):
    """é€†å¹³ç¨³å°æ³¢å˜æ¢(ISWT)é‡æ„æ¨¡å— - GPUåŠ é€Ÿç‰ˆæœ¬
    
    å°†å¤šé¢‘æ®µå°æ³¢ç³»æ•°é€šè¿‡é€†SWTé‡æ„å›æ—¶åŸŸä¿¡å·ã€‚
    ä¸SWTDecompositionå½¢æˆå¯¹ç§°çš„ç¼–ç -è§£ç æ¶æ„ã€‚
    
    ç‰¹ç‚¹ï¼š
    1. å®Œç¾é‡æ„ï¼ˆåœ¨ç†è®ºä¸Šå¯ä»¥å®Œå…¨æ¢å¤åŸä¿¡å·ï¼‰
    2. GPUåŠ é€Ÿï¼Œæ”¯æŒæ‰¹é‡å¤„ç†
    3. ä¸SWTDecompositionæ¥å£å¯¹ç§°
    
    Args:
        wavelet: å°æ³¢åŸºå‡½æ•°åç§°ï¼ˆéœ€ä¸åˆ†è§£æ—¶ä¸€è‡´ï¼‰
        level: SWTåˆ†è§£å±‚æ•°ï¼ˆéœ€ä¸åˆ†è§£æ—¶ä¸€è‡´ï¼‰
    
    Input:
        coeffs: (B, N, T, Level+1) - å¤šé¢‘æ®µå°æ³¢ç³»æ•°
                æœ€åä¸€ç»´çš„æ’åˆ—é¡ºåºï¼š[cA_n, cD_n, cD_{n-1}, ..., cD_1]
                ï¼ˆä¸SWTDecompositionè¾“å‡ºæ ¼å¼ä¸€è‡´ï¼‰
    
    Output:
        x: (B, N, T) - é‡æ„çš„æ—¶åŸŸä¿¡å·
    
    ç¤ºä¾‹ï¼š
        >>> # åˆ†è§£
        >>> swt = SWTDecomposition(wavelet='db4', level=3)
        >>> x = torch.randn(8, 7, 512)
        >>> coeffs = swt(x)  # (8, 7, 512, 4)
        >>> 
        >>> # é‡æ„
        >>> iswt = ISWTReconstruction(wavelet='db4', level=3)
        >>> x_recon = iswt(coeffs)  # (8, 7, 512)
        >>> 
        >>> # éªŒè¯é‡æ„è¯¯å·®
        >>> error = torch.abs(x - x_recon).mean()
        >>> print(f"é‡æ„è¯¯å·®: {error:.6f}")  # åº”è¯¥æ¥è¿‘0
    """
    
    def __init__(self, 
                 wavelet: str = 'db4', 
                 level: int = 3):
        super(ISWTReconstruction, self).__init__()
        
        # æ£€æŸ¥ptwtåº“æ˜¯å¦å¯ç”¨
        if not PTWT_AVAILABLE:
            raise ImportError(
                "ptwtåº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨GPUåŠ é€Ÿçš„ISWTã€‚\n"
                "è¯·è¿è¡Œ: pip install ptwt"
            )
        
        self.wavelet_name = wavelet
        self.level = level
        self.num_bands = level + 1
        
        # éªŒè¯å°æ³¢åç§°
        valid_wavelets = [
            'haar', 'db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8',
            'sym2', 'sym3', 'sym4', 'sym5', 'sym6', 'sym7', 'sym8',
            'coif1', 'coif2', 'coif3', 'coif4', 'coif5'
        ]
        if wavelet not in valid_wavelets:
            warnings.warn(
                f"å°æ³¢ '{wavelet}' å¯èƒ½ä¸è¢«ptwtæ”¯æŒã€‚\n"
                f"å¸¸ç”¨çš„å°æ³¢ç±»å‹: {', '.join(valid_wavelets[:10])}..."
            )
    
    def _validate_input(self, coeffs: torch.Tensor) -> None:
        """éªŒè¯è¾“å…¥å°æ³¢ç³»æ•°çš„åˆæ³•æ€§
        
        Args:
            coeffs: è¾“å…¥å°æ³¢ç³»æ•° (B, N, T, Level+1)
        
        Raises:
            ValueError: å¦‚æœè¾“å…¥ä¸æ»¡è¶³è¦æ±‚
        """
        if coeffs.ndim != 4:
            raise ValueError(
                f"è¾“å…¥å¿…é¡»æ˜¯4ç»´å¼ é‡ (Batch, N_vars, Time, Bands)ï¼Œå½“å‰ç»´åº¦: {coeffs.ndim}"
            )
        
        B, N, T, num_bands = coeffs.shape
        
        # æ£€æŸ¥é¢‘æ®µæ•°
        if num_bands != self.num_bands:
            raise ValueError(
                f"é¢‘æ®µæ•°ä¸åŒ¹é…ï¼šæœŸæœ› {self.num_bands}ï¼Œå®é™… {num_bands}"
            )
        
        # æ£€æŸ¥åºåˆ—é•¿åº¦
        min_length = 2 ** self.level
        if T < min_length:
            raise ValueError(
                f"åºåˆ—é•¿åº¦ {T} å¤ªçŸ­ï¼ŒISWT({self.level}å±‚)è‡³å°‘éœ€è¦ {min_length} ä¸ªæ—¶é—´æ­¥"
            )
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«NaNæˆ–Inf
        if torch.isnan(coeffs).any() or torch.isinf(coeffs).any():
            raise ValueError("è¾“å…¥åŒ…å«NaNæˆ–Infå€¼ï¼Œæ— æ³•è¿›è¡Œé‡æ„")
    
    def forward(self, coeffs: torch.Tensor) -> torch.Tensor:
        """
        æ‰§è¡Œé€†å¹³ç¨³å°æ³¢å˜æ¢
        
        Args:
            coeffs: (B, N, T, Level+1) - å°æ³¢ç³»æ•°
                    B: batchå¤§å°
                    N: å˜é‡æ•°é‡
                    T: æ—¶é—´æ­¥é•¿åº¦
                    Level+1: é¢‘æ®µæ•°é‡ [cA_n, cD_n, ..., cD_1]
        
        Returns:
            x: (B, N, T) - é‡æ„çš„æ—¶åŸŸä¿¡å·
        
        ç»´åº¦å˜æ¢æµç¨‹ï¼š
            (B, N, T, Level+1)
            -> Reshape -> (B*N, T, Level+1)
            -> é‡æ’ç³»æ•°é¡ºåºä¸ºptwtæ ¼å¼
            -> ptwt.iswt -> (B*N, T)
            -> Reshape -> (B, N, T)
        """
        # 1. è¾“å…¥éªŒè¯
        self._validate_input(coeffs)
        
        B, N, T, num_bands = coeffs.shape
        device = coeffs.device
        dtype = coeffs.dtype
        
        # 2. æ•°æ®ç±»å‹è½¬æ¢ï¼šptwtä¸æ”¯æŒbfloat16
        if dtype == torch.bfloat16:
            coeffs = coeffs.float()
            convert_back_to_bfloat16 = True
        else:
            convert_back_to_bfloat16 = False
        
        # 3. Reshape: (B, N, T, Level+1) -> (B*N, T, Level+1)
        coeffs_reshaped = coeffs.reshape(B * N, T, num_bands)
        
        # 4. é‡æ’ç³»æ•°é¡ºåºä»¥åŒ¹é…ptwt.iswtçš„æ ¼å¼
        # è¾“å…¥æ ¼å¼: [cA_n, cD_n, cD_{n-1}, ..., cD_1]
        # ptwt.iswtæœŸæœ›: [cD_1, cD_2, ..., cD_n, cA_n]
        coeffs_list = []
        
        # ä»åå¾€å‰å–ç»†èŠ‚ç³»æ•° (cD_1, cD_2, ..., cD_n)
        for i in range(num_bands - 1, 0, -1):
            coeffs_list.append(coeffs_reshaped[:, :, i])
        
        # æœ€åæ·»åŠ è¿‘ä¼¼ç³»æ•° (cA_n)
        coeffs_list.append(coeffs_reshaped[:, :, 0])
        
        # 5. æ‰§è¡ŒISWTï¼ˆGPUåŠ é€Ÿï¼‰
        try:
            x_reconstructed = ptwt.iswt(
                coeffs_list,         # list of tensors: [cD1, cD2, ..., cDn, cAn]
                self.wavelet_name    # å°æ³¢åç§°å­—ç¬¦ä¸²
            )  # è¾“å‡º: (B*N, T)
            
        except Exception as e:
            raise RuntimeError(
                f"ISWTé‡æ„å¤±è´¥: {e}\n"
                f"ç³»æ•°å½¢çŠ¶: {[c.shape for c in coeffs_list]}, "
                f"å°æ³¢: {self.wavelet_name}, å±‚æ•°: {self.level}"
            )
        
        # 6. Reshapeå›åŸå§‹batchç»“æ„
        # (B*N, T) -> (B, N, T)
        x_reconstructed = x_reconstructed.reshape(B, N, T)
        
        # 7. æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(x_reconstructed).any():
            warnings.warn(
                "ISWTé‡æ„ç»“æœåŒ…å«NaNå€¼ï¼Œå¯èƒ½æ˜¯è¾“å…¥ç³»æ•°å¼‚å¸¸"
            )
        
        # 8. è½¬å›åŸå§‹æ•°æ®ç±»å‹
        if convert_back_to_bfloat16:
            x_reconstructed = x_reconstructed.bfloat16()
        
        return x_reconstructed


class WaveletPatchEmbedding(nn.Module):
    """åŸºäºå¹³ç¨³å°æ³¢å˜æ¢çš„Patch Embeddingæ¨¡å— - ç›´æ¥æ‹¼æ¥æ³•
    
    å®ç°æ–¹æ¡ˆ1ï¼šå°†SWTåˆ†è§£åçš„å¤šä¸ªé¢‘æ®µåœ¨ç‰¹å¾ç»´åº¦ç›´æ¥æ‹¼æ¥ï¼Œç„¶åç»Ÿä¸€è¿›è¡ŒPatchingã€‚
    
    æ¶æ„æµç¨‹ï¼š
    1. å…¨å±€SWTåˆ†è§£ï¼šæå–å¤šå°ºåº¦é¢‘åŸŸç‰¹å¾
    2. é¢‘æ®µæ‹¼æ¥ï¼šå°†æ‰€æœ‰é¢‘æ®µstackåœ¨é€šé“ç»´åº¦
    3. ç»Ÿä¸€Patchingï¼šå¯¹æ‹¼æ¥åçš„æ•°æ®è¿›è¡Œpatchåˆ‡åˆ†
    4. æŠ•å½±é™ç»´ï¼šæ˜ å°„åˆ°ç›®æ ‡ç»´åº¦
    
    Args:
        d_model: è¾“å‡ºembeddingç»´åº¦
        patch_len: patché•¿åº¦ï¼ˆæ¨è16ï¼‰
        stride: patchæ»‘åŠ¨æ­¥é•¿ï¼ˆæ¨è8ï¼‰
        wavelet: å°æ³¢åŸºå‡½æ•° (é»˜è®¤'db4')
        level: SWTåˆ†è§£å±‚æ•° (é»˜è®¤3ï¼Œäº§ç”Ÿ4ä¸ªé¢‘æ®µ)
        dropout: dropoutç‡
    
    Input:
        x: (B, N, T) - [batch_size, num_variables, time_steps]
    
    Output:
        (B*N, num_patches, d_model), num_variables
    
    ç»´åº¦æµè½¬ç¤ºä¾‹ï¼š
        è¾“å…¥: (8, 7, 512)
        â†“ SWTåˆ†è§£
        (8, 7, 512, 4)  # 4ä¸ªé¢‘æ®µ
        â†“ é‡æ’ä¸ºå¤šé€šé“
        (8, 28, 512)  # 7*4=28ä¸ª"é€šé“"
        â†“ Padding
        (8, 28, 520)
        â†“ Unfold
        (8, 28, 64, 16)  # 64ä¸ªpatchesï¼Œæ¯ä¸ªé•¿åº¦16
        â†“ Reshape
        (224, 64, 16)  # 8*28=224
        â†“ Permute + Conv1dæŠ•å½±
        (224, 64, 32)  # æŠ•å½±åˆ°d_model=32
        â†“ é‡ç»„å›åŸå§‹å˜é‡ç»“æ„
        (56, 64, 32)  # 8*7=56
        â†“ æœ€ç»ˆè¾“å‡º
        output: (56, 64, 32), n_vars: 7
    """
    
    def __init__(self, 
                 d_model: int,
                 patch_len: int,
                 stride: int,
                 wavelet: str = 'db4',
                 level: int = 3,
                 dropout: float = 0.1):
        super(WaveletPatchEmbedding, self).__init__()
        
        self.d_model = d_model
        self.patch_len = patch_len
        self.stride = stride
        self.level = level
        self.num_bands = level + 1  # è¿‘ä¼¼ç³»æ•° + levelä¸ªç»†èŠ‚ç³»æ•°
        
        # 1. SWTåˆ†è§£æ¨¡å—
        self.swt = SWTDecomposition(wavelet=wavelet, level=level)
        
        # 2. Paddingå±‚ï¼ˆç”¨äºpatchingï¼‰
        self.padding_patch_layer = ReplicationPad1d((0, stride))
        
        # 3. æŠ•å½±å±‚ï¼šå°†patch_lenç»´åº¦æŠ•å½±åˆ°d_model
        # è¾“å…¥: (B*N*num_bands, patch_len, num_patches)
        # è¾“å‡º: (B*N*num_bands, d_model, num_patches)
        padding = 1 if torch.__version__ >= '1.5.0' else 2
        self.value_embedding = nn.Conv1d(
            in_channels=patch_len,
            out_channels=d_model,
            kernel_size=3,
            padding=padding,
            padding_mode='circular',
            bias=False
        )
        
        # åˆå§‹åŒ–æƒé‡
        nn.init.kaiming_normal_(
            self.value_embedding.weight, 
            mode='fan_in', 
            nonlinearity='leaky_relu'
        )
        
        # 4. Dropout
        self.dropout = nn.Dropout(dropout)
        
        # å‚æ•°éªŒè¯
        self._validate_params()
    
    def _validate_params(self):
        """å‚æ•°éªŒè¯"""
        assert self.patch_len > 0, "patch_lenå¿…é¡»å¤§äº0"
        assert self.stride > 0, "strideå¿…é¡»å¤§äº0"
        assert self.d_model > 0, "d_modelå¿…é¡»å¤§äº0"
        assert self.level >= 1, "levelå¿…é¡»è‡³å°‘ä¸º1"
        
        min_seq_len = 2 ** self.level
        if self.patch_len < min_seq_len:
            warnings.warn(
                f"patch_len ({self.patch_len}) å°äºSWTæœ€å°é•¿åº¦ ({min_seq_len}), "
                f"å¯èƒ½å¯¼è‡´è¾¹ç•Œæ•ˆåº”"
            )
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, int]:
        """
        Args:
            x: (B, N, T) - è¾“å…¥æ—¶é—´åºåˆ—
               B: batchå¤§å°
               N: å˜é‡æ•°é‡
               T: æ—¶é—´æ­¥é•¿åº¦
        
        Returns:
            output: (B*N, num_patches, d_model) - Patch embeddings
            n_vars: N - å˜é‡æ•°é‡ï¼ˆç”¨äºåç»­reshapeï¼‰
        
        ç»´åº¦å˜æ¢è¯¦ç»†æµç¨‹ï¼š
            (B, N, T) 
            -> SWTåˆ†è§£ -> (B, N, T, Level+1)
            -> Permute -> (B, N, Level+1, T)
            -> Reshape -> (B, N*(Level+1), T)
            -> Padding -> (B, N*(Level+1), T+stride)
            -> Unfold -> (B, N*(Level+1), num_patches, patch_len)
            -> Reshape -> (B*N*(Level+1), num_patches, patch_len)
            -> Permute -> (B*N*(Level+1), patch_len, num_patches)
            -> Conv1d -> (B*N*(Level+1), d_model, num_patches)
            -> Permute -> (B*N*(Level+1), num_patches, d_model)
            -> Reshape -> (B*N, num_patches, d_model*(Level+1))
            -> Mean -> (B*N, num_patches, d_model)
        """
        B, N, T = x.shape
        n_vars = N  # ä¿å­˜åŸå§‹å˜é‡æ•°
        
        # ===== Step 1: SWTå…¨å±€åˆ†è§£ =====
        # (B, N, T) -> (B, N, T, Level+1)
        swt_coeffs = self.swt(x)
        
        # ===== Step 2: é‡æ’ç»´åº¦ï¼Œå°†é¢‘æ®µä½œä¸º"é€šé“" =====
        # (B, N, T, Level+1) -> (B, N, Level+1, T)
        swt_coeffs = swt_coeffs.permute(0, 1, 3, 2).contiguous()
        
        # Reshape: (B, N, Level+1, T) -> (B, N*(Level+1), T)
        # å°†é¢‘æ®µå±•å¼€åˆ°å˜é‡ç»´åº¦ï¼Œç›¸å½“äºæŠŠæ¯ä¸ªé¢‘æ®µå½“ä½œç‹¬ç«‹çš„"å˜é‡"
        x_multi_band = swt_coeffs.reshape(B, N * self.num_bands, T)
        
        # ===== Step 3: Padding =====
        # (B, N*(Level+1), T) -> (B, N*(Level+1), T+stride)
        x_padded = self.padding_patch_layer(x_multi_band)
        
        # ===== Step 4: Unfoldè¿›è¡ŒPatching =====
        # (B, N*(Level+1), T+stride) -> (B, N*(Level+1), num_patches, patch_len)
        x_patches = x_padded.unfold(
            dimension=-1,
            size=self.patch_len,
            step=self.stride
        )
        
        num_patches = x_patches.shape[2]
        
        # ===== Step 5: Reshapeåˆå¹¶batchå’Œé€šé“ç»´åº¦ =====
        # (B, N*(Level+1), num_patches, patch_len) 
        # -> (B*N*(Level+1), num_patches, patch_len)
        x_patches_flat = x_patches.reshape(
            B * N * self.num_bands,
            num_patches,
            self.patch_len
        )
        
        # ===== Step 6: æŠ•å½±åˆ°d_modelç»´åº¦ =====
        # Conv1dæœŸæœ›è¾“å…¥: (Batch, Channels, Length)
        # éœ€è¦permute: (B*N*(Level+1), num_patches, patch_len)
        #         -> (B*N*(Level+1), patch_len, num_patches)
        x_permuted = x_patches_flat.permute(0, 2, 1).contiguous()
        
        # Conv1dæŠ•å½±
        # (B*N*(Level+1), patch_len, num_patches) 
        # -> (B*N*(Level+1), d_model, num_patches)
        x_embedded = self.value_embedding(x_permuted)
        
        # è½¬å›: (B*N*(Level+1), d_model, num_patches)
        #   -> (B*N*(Level+1), num_patches, d_model)
        x_embedded = x_embedded.transpose(1, 2)
        
        # ===== Step 7: é‡ç»„å›åŸå§‹å˜é‡ç»“æ„ =====
        # (B*N*(Level+1), num_patches, d_model)
        # -> (B, N, Level+1, num_patches, d_model)
        x_reshaped = x_embedded.reshape(
            B, N, self.num_bands, num_patches, self.d_model
        )
        
        # ===== Step 8: é¢‘æ®µç‹¬ç«‹æ€§ä¿æŒï¼ˆç¼–ç -è§£ç å¯¹ç§°ä¼˜åŒ–ï¼‰=====
        # âš ï¸ å…³é”®æ”¹åŠ¨ï¼šä¸å†ä½¿ç”¨ç®€å•å¹³å‡èåˆï¼
        # åŸæ–¹æ¡ˆï¼ˆä¸å¯¹ç§°ï¼‰ï¼š
        #   ç¼–ç ï¼š4é¢‘æ®µ â†’ meanèåˆ â†’ 1æ··åˆå‘é‡
        #   è§£ç ï¼š1æ··åˆå‘é‡ â†’ åˆ†ç¦» â†’ 4é¢‘æ®µ  âŒ ä¿¡æ¯ç“¶é¢ˆ
        #
        # æ–°æ–¹æ¡ˆï¼ˆå¯¹ç§°ï¼‰ï¼š
        #   ç¼–ç ï¼š4é¢‘æ®µ â†’ ä¿æŒç‹¬ç«‹ â†’ 4é¢‘æ®µç‰¹å¾
        #   è§£ç ï¼š4é¢‘æ®µç‰¹å¾ â†’ ç‹¬ç«‹é¢„æµ‹ â†’ 4é¢‘æ®µ  âœ… ä¿¡æ¯æ— æŸ
        #
        # (B, N, num_bands, num_patches, d_model)
        # -> (B, N, num_patches, num_bands*d_model)
        # å°†é¢‘æ®µç»´åº¦å±•å¹³åˆ°ç‰¹å¾ç»´åº¦ï¼Œè€Œä¸æ˜¯å¹³å‡æ‰
        x_multiband = x_reshaped.permute(0, 1, 3, 2, 4).contiguous()
        # (B, N, num_patches, num_bands, d_model)
        
        x_multiband = x_multiband.reshape(
            B, N, num_patches, self.num_bands * self.d_model
        )
        # (B, N, num_patches, 4*d_model)  ä¾‹å¦‚ï¼š4*32=128ç»´
        
        # ===== Step 9: æœ€ç»ˆreshape =====
        # (B, N, num_patches, num_bands*d_model) 
        # -> (B*N, num_patches, num_bands*d_model)
        output = x_multiband.reshape(B * N, num_patches, self.num_bands * self.d_model)
        
        # ===== Step 10: Dropout =====
        output = self.dropout(output)
        
        # print(f"[WaveletPatchEmbedding] ç¼–ç -è§£ç å¯¹ç§°è®¾è®¡ï¼š")
        # print(f"  è¾“å‡ºç»´åº¦: {output.shape}")
        # print(f"  é¢‘æ®µæ•°: {self.num_bands}, æ¯é¢‘æ®µç»´åº¦: {self.d_model}")
        # print(f"  æ€»ç‰¹å¾ç»´åº¦: {self.num_bands * self.d_model} (ä¿æŒé¢‘æ®µç‹¬ç«‹)")
        # print(f"  âœ… ä¿¡æ¯æ— æŸä¼ é€’ï¼Œä¸è§£ç ç«¯å®Œå…¨å¯¹ç§°")
        
        return output, n_vars


if __name__ == "__main__":
    """æµ‹è¯•ä»£ç """
    print("=" * 80)
    print("æµ‹è¯• SWTDecomposition æ¨¡å—")
    print("=" * 80)
    
    # æ£€æŸ¥ptwtæ˜¯å¦å¯ç”¨
    if not PTWT_AVAILABLE:
        print("\nâŒ ptwtåº“æœªå®‰è£…ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        print("è¯·è¿è¡Œ: pip install ptwt")
        exit(1)
    
    # æµ‹è¯•å‚æ•°
    batch_size = 4
    num_vars = 7
    seq_len = 512
    level = 3
    wavelet = 'db4'
    
    print(f"\næµ‹è¯•é…ç½®:")
    print(f"  - Batch Size: {batch_size}")
    print(f"  - Variables: {num_vars}")
    print(f"  - Sequence Length: {seq_len}")
    print(f"  - Wavelet: {wavelet}")
    print(f"  - Level: {level}")
    
    # åˆ›å»ºæ¨¡å‹
    swt = SWTDecomposition(wavelet=wavelet, level=level)
    
    # å¼ºåˆ¶ä½¿ç”¨0å·æ˜¾å¡
    if torch.cuda.is_available():
        device = torch.device('cuda:0')
        print(f"  - Device: {device} (å¼ºåˆ¶ä½¿ç”¨0å·æ˜¾å¡)")
    else:
        device = torch.device('cpu')
        print(f"  - Device: {device} (CPUæ¨¡å¼)")
    
    swt = swt.to(device)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(batch_size, num_vars, seq_len, device=device)
    
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"  -> (Batch={x.shape[0]}, Vars={x.shape[1]}, Time={x.shape[2]})")
    
    # å‰å‘ä¼ æ’­
    print("\næ‰§è¡ŒSWTåˆ†è§£...")
    coeffs = swt(x)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶: {coeffs.shape}")
    print(f"  -> (Batch={coeffs.shape[0]}, Vars={coeffs.shape[1]}, "
          f"Time={coeffs.shape[2]}, Bands={coeffs.shape[3]})")
    
    # åˆ†æå„é¢‘æ®µ
    print(f"\né¢‘æ®µåˆ†æ:")
    print(f"  - è¿‘ä¼¼ç³»æ•° cA{level} (ä½é¢‘è¶‹åŠ¿): coeffs[..., 0]")
    for i in range(1, level + 1):
        print(f"  - ç»†èŠ‚ç³»æ•° cD{level - i + 1} (é¢‘æ®µ{i}): coeffs[..., {i}]")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    for i in range(level + 1):
        band = coeffs[..., i]
        band_name = f"cA{level}" if i == 0 else f"cD{level - i + 1}"
        print(f"  {band_name}:")
        print(f"    Mean: {band.mean().item():.6f}")
        print(f"    Std:  {band.std().item():.6f}")
        print(f"    Min:  {band.min().item():.6f}")
        print(f"    Max:  {band.max().item():.6f}")
    
    # æµ‹è¯•ReplicationPad1d
    print("\n" + "=" * 80)
    print("æµ‹è¯• ReplicationPad1d æ¨¡å—")
    print("=" * 80)
    
    stride = 8
    pad = ReplicationPad1d((0, stride))
    x_padded = pad(x)
    
    print(f"\nåŸå§‹å½¢çŠ¶: {x.shape}")
    print(f"å¡«å……å: {x_padded.shape}")
    print(f"å¡«å……é•¿åº¦: {x_padded.shape[-1] - x.shape[-1]}")
    
    # éªŒè¯å¡«å……æ­£ç¡®æ€§
    is_correct = torch.allclose(
        x_padded[:, :, -stride:], 
        x[:, :, -1:].repeat(1, 1, stride)
    )
    print(f"å¡«å……æ­£ç¡®æ€§: {'âœ… é€šè¿‡' if is_correct else 'âŒ å¤±è´¥'}")
    
    print("\n" + "=" * 80)
    print("æµ‹è¯• WaveletPatchEmbedding æ¨¡å—")
    print("=" * 80)
    
    # æµ‹è¯•å‚æ•°ï¼ˆä¸TimeLLMé…ç½®ä¸€è‡´ï¼‰
    d_model = 32
    patch_len = 16
    stride = 8
    
    print(f"\næµ‹è¯•é…ç½®:")
    print(f"  - d_model: {d_model}")
    print(f"  - patch_len: {patch_len}")
    print(f"  - stride: {stride}")
    print(f"  - wavelet: {wavelet}")
    print(f"  - level: {level}")
    
    # åˆ›å»ºæ¨¡å‹
    wavelet_patch_embed = WaveletPatchEmbedding(
        d_model=d_model,
        patch_len=patch_len,
        stride=stride,
        wavelet=wavelet,
        level=level,
        dropout=0.1
    )
    
    wavelet_patch_embed = wavelet_patch_embed.to(device)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥ï¼ˆä¸åŸå§‹PatchEmbeddingè¾“å…¥æ ¼å¼ä¸€è‡´ï¼‰
    x_test = torch.randn(batch_size, num_vars, seq_len, device=device)
    
    print(f"\nè¾“å…¥å½¢çŠ¶: {x_test.shape}")
    print(f"  -> (Batch={x_test.shape[0]}, Vars={x_test.shape[1]}, Time={x_test.shape[2]})")
    
    # å‰å‘ä¼ æ’­
    print("\næ‰§è¡ŒWaveletPatchEmbedding...")
    output, n_vars_out = wavelet_patch_embed(x_test)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"  -> (Batch*Vars={output.shape[0]}, Patches={output.shape[1]}, D_model={output.shape[2]})")
    print(f"å˜é‡æ•°: {n_vars_out}")
    
    # è®¡ç®—é¢„æœŸçš„patchæ•°é‡
    expected_patches = int((seq_len - patch_len) / stride + 2)
    actual_patches = output.shape[1]
    print(f"\né¢„æœŸPatchæ•°: {expected_patches}")
    print(f"å®é™…Patchæ•°: {actual_patches}")
    
    # éªŒè¯è¾“å‡ºæ ¼å¼
    print(f"\næ ¼å¼éªŒè¯:")
    expected_shape_0 = batch_size * num_vars
    expected_shape_2 = d_model
    
    check1 = output.shape[0] == expected_shape_0
    check2 = output.shape[2] == expected_shape_2
    check3 = n_vars_out == num_vars
    
    print(f"  âœ… Batch*Varsç»´åº¦æ­£ç¡®: {check1} ({output.shape[0]} == {expected_shape_0})")
    print(f"  âœ… D_modelç»´åº¦æ­£ç¡®: {check2} ({output.shape[2]} == {expected_shape_2})")
    print(f"  âœ… å˜é‡æ•°è¿”å›æ­£ç¡®: {check3} ({n_vars_out} == {num_vars})")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nè¾“å‡ºç»Ÿè®¡:")
    print(f"  Mean: {output.mean().item():.6f}")
    print(f"  Std:  {output.std().item():.6f}")
    print(f"  Min:  {output.min().item():.6f}")
    print(f"  Max:  {output.max().item():.6f}")
    
    # å‚æ•°é‡å¯¹æ¯”
    print(f"\nå‚æ•°é‡ç»Ÿè®¡:")
    
    # è®¡ç®—SWTDecompositionå‚æ•°ï¼ˆå®é™…ä¸º0ï¼Œå› ä¸ºæ˜¯å›ºå®šå˜æ¢ï¼‰
    swt_params = sum(p.numel() for p in wavelet_patch_embed.swt.parameters())
    embed_params = sum(p.numel() for p in wavelet_patch_embed.value_embedding.parameters())
    total_params = sum(p.numel() for p in wavelet_patch_embed.parameters())
    
    print(f"  - SWTåˆ†è§£å±‚: {swt_params:,} å‚æ•°")
    print(f"  - æŠ•å½±å±‚: {embed_params:,} å‚æ•°")
    print(f"  - æ€»å‚æ•°é‡: {total_params:,} å‚æ•°")
    
    # ä¸åŸå§‹PatchEmbeddingå¯¹æ¯”
    print(f"\nä¸åŸå§‹PatchEmbeddingå¯¹æ¯”:")
    print(f"  - åŸå§‹: TokenEmbedding(patch_len={patch_len}, d_model={d_model})")
    print(f"        å‚æ•°é‡ â‰ˆ {patch_len * d_model * 3:,}")  # Conv1d kernel_size=3
    print(f"  - Wavelet: å‚æ•°é‡ = {total_params:,}")
    print(f"  - å¢åŠ æ¯”ä¾‹: {(total_params / (patch_len * d_model * 3) - 1) * 100:.1f}%")
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
    
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¥å£å…¼å®¹æ€§æµ‹è¯•")
    print("=" * 80)
    print("\nâœ… WaveletPatchEmbedding ä¸ PatchEmbedding æ¥å£å®Œå…¨å…¼å®¹!")
    print("å¯ä»¥ç›´æ¥æ›¿æ¢TimeLLMä¸­çš„patch_embeddingæ¨¡å—")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("  from layers.WaveletEmbed import WaveletPatchEmbedding")
    print("  self.patch_embedding = WaveletPatchEmbedding(")
    print("      d_model=configs.d_model,")
    print("      patch_len=self.patch_len,")
    print("      stride=self.stride,")
    print("      wavelet='db4',")
    print("      level=3,")
    print("      dropout=configs.dropout")
    print("  )")
    print("=" * 80)
